#!/usr/bin/env python3
"""
BetExplorer Scraper pro historick√© kurzy NHL
Stahuje historick√© s√°zkov√© kurzy z betexplorer.com pro NHL z√°pasy.

Autor: Hockey Prediction System
Datum: 2025
"""

import requests
from bs4 import BeautifulSoup, Comment
import pandas as pd
import time
import os
from datetime import datetime, timedelta, date
import logging
from typing import List, Dict, Optional, Tuple
import re
from urllib.parse import urljoin, urlparse
import json
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options
from selenium.common.exceptions import TimeoutException, NoSuchElementException
import random

# Setup logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('logs/betexplorer_scraper.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class BetExplorerScraper:
    """Scraper pro historick√© kurzy z BetExplorer.com"""
    
    def __init__(self, use_selenium: bool = True, headless: bool = True):
        """
        Inicializace scraperu
        
        Args:
            use_selenium: Pou≈æ√≠t Selenium pro dynamick√Ω obsah
            headless: Spustit prohl√≠≈æeƒç v headless re≈æimu
        """
        self.base_url = "https://www.betexplorer.com"
        self.use_selenium = use_selenium
        
        # HTTP session for basic requests
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'cs,en-US;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        })
        
        # Selenium setup
        self.driver = None
        if self.use_selenium:
            self._setup_selenium(headless)
        
        # Rate limiting
        self.delay_range = (2, 5)  # Random delays between requests
        self.last_request_time = 0
        
        # Team mapping (betexplorer name -> our name)
        self.team_mapping = self._load_team_mapping()
        
        # Cache for results
        self.cache = {}
        
    def _setup_selenium(self, headless: bool = True):
        """Nastaven√≠ Selenium WebDriveru"""
        try:
            chrome_options = Options()
            if headless:
                chrome_options.add_argument('--headless')
            chrome_options.add_argument('--no-sandbox')
            chrome_options.add_argument('--disable-dev-shm-usage')
            chrome_options.add_argument('--disable-gpu')
            chrome_options.add_argument('--window-size=1920,1080')
            chrome_options.add_argument('--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')
            
            self.driver = webdriver.Chrome(options=chrome_options)
            self.driver.implicitly_wait(10)
            
            logger.info("‚úÖ Selenium WebDriver √∫spƒõ≈°nƒõ inicializov√°n")
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Nepoda≈ôilo se inicializovat Selenium: {e}")
            logger.warning("Pokraƒçuji pouze s requests...")
            self.use_selenium = False
            self.driver = None
    
    def _load_team_mapping(self) -> Dict[str, str]:
        """Naƒçte mapov√°n√≠ n√°zv≈Ø t√Ωm≈Ø"""
        # Basic mapping of NHL teams (betexplorer -> standard name)
        return {
            'Anaheim Ducks': 'Anaheim Ducks',
            'Arizona Coyotes': 'Arizona Coyotes', 
            'Boston Bruins': 'Boston Bruins',
            'Buffalo Sabres': 'Buffalo Sabres',
            'Calgary Flames': 'Calgary Flames',
            'Carolina Hurricanes': 'Carolina Hurricanes',
            'Chicago Blackhawks': 'Chicago Blackhawks',
            'Colorado Avalanche': 'Colorado Avalanche',
            'Columbus Blue Jackets': 'Columbus Blue Jackets',
            'Dallas Stars': 'Dallas Stars',
            'Detroit Red Wings': 'Detroit Red Wings',
            'Edmonton Oilers': 'Edmonton Oilers',
            'Florida Panthers': 'Florida Panthers',
            'Los Angeles Kings': 'Los Angeles Kings',
            'Minnesota Wild': 'Minnesota Wild',
            'Montreal Canadiens': 'Montreal Canadiens',
            'Nashville Predators': 'Nashville Predators',
            'New Jersey Devils': 'New Jersey Devils',
            'New York Islanders': 'New York Islanders',
            'New York Rangers': 'New York Rangers',
            'Ottawa Senators': 'Ottawa Senators',
            'Philadelphia Flyers': 'Philadelphia Flyers',
            'Pittsburgh Penguins': 'Pittsburgh Penguins',
            'San Jose Sharks': 'San Jose Sharks',
            'Seattle Kraken': 'Seattle Kraken',
            'St. Louis Blues': 'St. Louis Blues',
            'Tampa Bay Lightning': 'Tampa Bay Lightning',
            'Toronto Maple Leafs': 'Toronto Maple Leafs',
            'Vancouver Canucks': 'Vancouver Canucks',
            'Vegas Golden Knights': 'Vegas Golden Knights',
            'Washington Capitals': 'Washington Capitals',
            'Winnipeg Jets': 'Winnipeg Jets'
        }
    
    def _rate_limit(self):
        """Implementuje rate limiting"""
        current_time = time.time()
        time_since_last = current_time - self.last_request_time
        
        delay = random.uniform(*self.delay_range)
        if time_since_last < delay:
            sleep_time = delay - time_since_last
            logger.debug(f"Rate limiting: ƒçek√°m {sleep_time:.1f}s")
            time.sleep(sleep_time)
        
        self.last_request_time = time.time()
    
    def _make_request(self, url: str, use_selenium: bool = None) -> Optional[BeautifulSoup]:
        """
        Provede HTTP po≈æadavek s rate limitingem
        
        Args:
            url: URL pro sta≈æen√≠
            use_selenium: Pou≈æ√≠t Selenium m√≠sto requests
        
        Returns:
            BeautifulSoup objekt nebo None
        """
        self._rate_limit()
        
        if use_selenium is None:
            use_selenium = self.use_selenium
        
        try:
            logger.debug(f"Stahuji: {url}")
            
            if use_selenium and self.driver:
                self.driver.get(url)
                
                # Wait for the page to load
                WebDriverWait(self.driver, 10).until(
                    EC.presence_of_element_located((By.TAG_NAME, "body"))
                )
                
                html = self.driver.page_source
                return BeautifulSoup(html, 'html.parser')
            
            else:
                response = self.session.get(url, timeout=30)
                response.raise_for_status()
                return BeautifulSoup(response.content, 'html.parser')
                
        except Exception as e:
            logger.error(f"Chyba p≈ôi stahov√°n√≠ {url}: {e}")
            return None
    
    def get_season_results_urls(self, season: str) -> List[str]:
        """
        Z√≠sk√° URL v≈°ech v√Ωsledk≈Ø pro danou sez√≥nu
        
        Args:
            season: Sez√≥na ve form√°tu '2021-2022'
        
        Returns:
            Seznam URL jednotliv√Ωch z√°pas≈Ø
        """
        # Convert season to betexplorer format
        #season_formatted = season.replace('-', '-20') if len(season.split('-')[0]) == 4 else season
        season_url = f"{self.base_url}/hockey/usa/nhl-{season}/results/"
        
        logger.info(f"Naƒç√≠t√°m v√Ωsledky pro sez√≥nu {season}")
        
        soup = self._make_request(season_url)
        if not soup:
            logger.error(f"Nepoda≈ôilo se naƒç√≠st v√Ωsledky pro sez√≥nu {season}")
            return []

        # Najdi element s title="Main season game statistics" a z√≠skej stage URL
        stage_element = soup.find(attrs={'title': 'Main season game statistics'})
        if not stage_element:
            logger.error("Nenalezen element s title='Main season game statistics'")
            return []
        
        # Z√≠skej href z elementu (m≈Ø≈æe b√Ωt buƒè p≈ô√≠mo href nebo v parent elementu)
        stage_href = None
        if stage_element.name == 'a' and stage_element.get('href'):
            stage_href = stage_element['href']
        else:
            # Hledej parent element, kter√Ω je link
            parent = stage_element.find_parent('a')
            if parent and parent.get('href'):
                stage_href = parent['href']
        
        if not stage_href:
            logger.error("Nenalezen href pro Main season game statistics")
            return []
        
        # Vytvo≈ô novou URL s &month=all
        stage_url = urljoin(season_url, stage_href)
        if '?' in stage_url:
            results_url = f"{stage_url}&month=all"
        else:
            results_url = f"{stage_url}?month=all"
        
        logger.info(f"Pou≈æ√≠v√°m stage URL: {results_url}")
        
        # Naƒçti str√°nku s kompletn√≠mi v√Ωsledky
        soup = self._make_request(results_url)
        if not soup:
            logger.error(f"Nepoda≈ôilo se naƒç√≠st kompletn√≠ v√Ωsledky pro sez√≥nu {season}")
            return []
        
        match_urls = []
        
        # Find the table with the results
        results_table = soup.find('table', {'class': 'table-main'})
        if not results_table:
            logger.error("Nenalezena tabulka s v√Ωsledky")
            return []
        
        # Extract links to individual matches
        for row in results_table.find_all('tr'):
            cells = row.find_all('td')
            if len(cells) >= 5:  # At least 5 columns expected
                # Search for a match link
                match_link = None
                for cell in cells:
                    link = cell.find('a')
                    if link and '/hockey/usa/nhl' in link.get('href', ''):
                        match_link = urljoin(self.base_url, link['href'])
                        break
                
                if match_link:
                    match_urls.append(match_link)
        
        logger.info(f"Nalezeno {len(match_urls)} z√°pas≈Ø pro sez√≥nu {season}")
        return match_urls
    
    def extract_match_odds(self, match_url: str) -> Optional[Dict]:
        """
        Extrahuje kurzy pro konkr√©tn√≠ z√°pas
        
        Args:
            match_url: URL z√°pasu na betexplorer
        
        Returns:
            Slovn√≠k s kurzy a informacemi o z√°pase
        """
        soup = self._make_request(match_url)
        if not soup:
            return None
        
        try:
            # Extract basic match information
            match_info = self._extract_match_info(soup, match_url)
            if not match_info:
                return None
            
            # Extract odds
            odds_data = self._extract_odds_from_page(soup, match_url)
            
            # Combine information
            result = {
                **match_info,
                'odds': odds_data,
                'scraped_at': datetime.now(),
                'source_url': match_url
            }
            
            return result
            
        except Exception as e:
            logger.error(f"Chyba p≈ôi extrakci kurz≈Ø z {match_url}: {e}")
            return None
    
    def _extract_match_info(self, soup: BeautifulSoup, match_url: str) -> Optional[Dict]:
        """Extrahuje z√°kladn√≠ informace o z√°pase"""
        try:
            # Extrahuj t√Ωmy z list-details struktury
            team_elements = soup.find_all('h2', {'class': 'list-details__item__title'})
            
            if len(team_elements) < 2:
                logger.error("Nenalezeny t√Ωmy na str√°nce")
                return None
            
            # Prvn√≠ t√Ωm je dom√°c√≠, druh√Ω je hostuj√≠c√≠ (podle po≈ôad√≠ v HTML)
            home_team = team_elements[0].get_text(strip=True)
            away_team = team_elements[1].get_text(strip=True)
            
            # Mapuj n√°zvy t√Ωm≈Ø
            home_team = self.team_mapping.get(home_team, home_team)
            away_team = self.team_mapping.get(away_team, away_team)
            
            # Extrahuj datum z data-dt atributu
            match_date = None
            date_elem = soup.find(attrs={'data-dt': True})
            if date_elem:
                date_string = date_elem.get('data-dt')
                # Format: "31,10,2021,21,00"
                date_parts = date_string.split(',')
                if len(date_parts) >= 3:
                    try:
                        day = int(date_parts[0])
                        month = int(date_parts[1])
                        year = int(date_parts[2])
                        match_date = date(year, month, day)
                    except (ValueError, IndexError):
                        logger.warning(f"Nepoda≈ôilo se parsovat datum: {date_string}")
            
            # Extrahuj v√Ωsledek z js-score elementu
            home_score, away_score = None, None
            score_elem = soup.find(id='js-score')
            if score_elem:
                score_text = score_elem.get_text(strip=True)
                if score_text and ':' in score_text:
                    try:
                        scores = score_text.split(':')
                        if len(scores) == 2:
                            home_score = int(scores[0].strip())
                            away_score = int(scores[1].strip())
                    except ValueError:
                        logger.warning(f"Nepoda≈ôilo se parsovat sk√≥re: {score_text}")
            
            # Urƒçen√≠ statusu z√°pasu
            status = 'scheduled'  # V√Ωchoz√≠ hodnota
            
            # Zkontroluj isFinished hodnotu
            finished_elem = soup.find(id='isFinished')
            if finished_elem:
                finished_value = finished_elem.get('value', '0')
                if finished_value == '1':
                    status = 'completed'
            
            # Zkontroluj isLive hodnotu
            live_elem = soup.find(id='isLive')
            if live_elem:
                live_value = live_elem.get('value', '')
                if live_value and live_value != '':
                    status = 'live'
            
            # Pokud m√°me sk√≥re ale status nen√≠ completed, pravdƒõpodobnƒõ je z√°pas dokonƒçen
            if home_score is not None and away_score is not None and status == 'scheduled':
                status = 'completed'
            
            # Extrahuj podrobnosti sk√≥re (periody) pokud existuj√≠
            partial_score = None
            partial_elem = soup.find(id='js-partial')
            if partial_elem:
                partial_score = partial_elem.get_text(strip=True)
            
            result = {
                'home_team': home_team,
                'away_team': away_team,
                'match_date': match_date,
                'home_score': home_score,
                'away_score': away_score,
                'status': status
            }
            
            # P≈ôidej podrobnosti sk√≥re pokud existuj√≠
            if partial_score:
                result['partial_score'] = partial_score
            
            logger.debug(f"Extrahovan√© informace: {result}")
            return result
            
        except Exception as e:
            logger.error(f"Chyba p≈ôi extrakci informac√≠ o z√°pase: {e}")
            return None

    def _extract_odds_from_page(self, soup: BeautifulSoup, match_url: str) -> List[Dict]:
        """
        Extrahuje kurzy ze str√°nky z√°pasu pomoc√≠ API
        
        Args:
            soup: BeautifulSoup objekt str√°nky (pro fallback)
            match_url: URL z√°pasu pro extrakci ID
        
        Returns:
            Seznam slovn√≠k≈Ø s kurzy
        """
        odds_data = []
        
        try:
            # Extrahuj match ID z URL
            match_id = self._extract_match_id_from_url(match_url)
            if not match_id:
                logger.error(f"Nepoda≈ôilo se extrahovat match ID z URL: {match_url}")
                return []
            
            logger.debug(f"Extrahov√°n match ID: {match_id}")
            
            # Z√≠skej kurzy pro r≈Øzn√© typy trh≈Ø
            market_types = [
                ('HA', 'moneyline_2way', 2),  # Dom√°c√≠/Host√© (2-way)
                # ('1x2', '1x2', 3)            # 1X2 (3-way) - pro √∫plnost
            ]
            
            for market_code, market_name, expected_odds_count in market_types:
                try:
                    market_odds = self._fetch_match_odds(match_id, market_code, market_name, expected_odds_count)
                    if market_odds:
                        odds_data.extend(market_odds)
                except Exception as e:
                    logger.warning(f"Chyba p≈ôi naƒç√≠t√°n√≠ kurz≈Ø pro trh {market_name}: {e}")
                    continue
            
            logger.info(f"Extrahov√°no {len(odds_data)} kurz≈Ø pro z√°pas {match_id}")
            
        except Exception as e:
            logger.error(f"Chyba p≈ôi extrakci kurz≈Ø: {e}")
        
        return odds_data
    
    def _extract_match_id_from_url(self, match_url: str) -> Optional[str]:
        """
        Extrahuje ID z√°pasu z URL
        
        Args:
            match_url: URL z√°pasu (nap≈ô. https://www.betexplorer.com/hockey/usa/nhl/buffalo-sabres-philadelphia-flyers/z1sKAia5/)
        
        Returns:
            8-znakov√© ID z√°pasu nebo None
        """
        try:
            # URL form√°t: .../team1-team2/MATCH_ID/
            # Nebo: .../team1-team2/MATCH_ID/odds/
            
            # Rozdƒõl URL podle '/'
            url_parts = match_url.rstrip('/').split('/')
            
            # Hledej 8-znakov√Ω identifik√°tor
            for part in reversed(url_parts):  # Zaƒçni od konce
                if len(part) == 8 and part.isalnum():
                    return part
            
            # Fallback: regex pro 8 znak≈Ø
            import re
            match_id_pattern = r'/([a-zA-Z0-9]{8})/?(?:odds/?)?$'
            match = re.search(match_id_pattern, match_url)
            if match:
                return match.group(1)
            
            logger.error(f"Match ID nenalezeno v URL: {match_url}")
            return None
            
        except Exception as e:
            logger.error(f"Chyba p≈ôi extrakci match ID: {e}")
            return None

    def _fetch_match_odds(self, match_id: str, market_code: str, market_name: str, expected_odds_count: int) -> List[Dict]:
        """Naƒçte kurzy pro konkr√©tn√≠ z√°pas a trh"""
        try:
            # Sestav API URL
            odds_url = f"{self.base_url}/match-odds-old/{match_id}/1/{market_code}/1/en/"
            
            logger.debug(f"Naƒç√≠t√°m kurzy z: {odds_url}")
            
            # Proveƒè API po≈æadavek
            response = self.session.get(odds_url, timeout=30)
            response.raise_for_status()
            
            # Parsuj JSON odpovƒõƒè
            try:
                data = response.json()
            except ValueError as e:
                logger.error(f"Neplatn√° JSON odpovƒõƒè z {odds_url}: {e}")
                return []
            
            # Zpracuj kurzy z odpovƒõdi
            if 'odds' not in data:
                logger.warning(f"Kl√≠ƒç 'odds' nenalezen v odpovƒõdi pro {market_name}")
                return []
            
            return self._parse_odds_response(data, market_name, expected_odds_count)
            
        except Exception as e:
            logger.error(f"Chyba p≈ôi naƒç√≠t√°n√≠ kurz≈Ø pro {market_name}: {e}")
            return []
        
    def _parse_odds_response(self, response_data: Dict, market_name: str, expected_odds_count: int) -> List[Dict]:
        """Parsuje JSON odpovƒõƒè s HTML obsahem kurz≈Ø"""
        odds_list = []
        
        try:
            # API vrac√≠ HTML obsah v kl√≠ƒçi "odds"
            html_content = response_data.get('odds', '')
            if not html_content:
                logger.warning(f"Pr√°zdn√Ω HTML obsah kurz≈Ø pro {market_name}")
                return []
            
            # Parsuj HTML obsah
            soup = BeautifulSoup(html_content, 'html.parser')
            
            # Najdi tabulku s kurzy
            odds_table = soup.find('table', {'class': 'table-main'})
            if not odds_table:
                logger.warning(f"Tabulka s kurzy nenalezena pro {market_name}")
                return []
            
            # Najdi ≈ô√°dky s kurzy v tbody
            tbody = odds_table.find('tbody')
            if not tbody:
                logger.warning(f"tbody tabulky nenalezen pro {market_name}")
                return []
            
            # Zpracuj ka≈æd√Ω ≈ô√°dek s bookmaker kurzy
            for row in tbody.find_all('tr'):
                try:
                    odds_entry = self._parse_bookmaker_row(row, market_name, expected_odds_count)
                    if odds_entry:
                        odds_list.append(odds_entry)
                except Exception as e:
                    logger.warning(f"Chyba p≈ôi zpracov√°n√≠ ≈ô√°dku kurzu: {e}")
                    continue
                
            # Zpracuj tak√© pr≈Ømƒõrn√© kurzy z tfoot
            try:
                average_odds = self._parse_average_odds(odds_table, market_name, expected_odds_count)
                if average_odds:
                    odds_list.append(average_odds)
            except Exception as e:
                logger.warning(f"Chyba p≈ôi zpracov√°n√≠ pr≈Ømƒõrn√Ωch kurz≈Ø: {e}")
            
            logger.debug(f"Parsov√°no {len(odds_list)} kurz≈Ø pro {market_name}")

        except Exception as e:
            logger.error(f"Chyba p≈ôi parsov√°n√≠ HTML kurz≈Ø: {e}")
        
        return odds_list
    
    def _parse_bookmaker_row(self, row, market_name: str, expected_odds_count: int) -> Optional[Dict]:
        """Parsuje ≈ô√°dek s kurzy konkr√©tn√≠ho bookmaker"""
        try:
            # Najdi n√°zev bookmaker - v odkazu nebo span elementu
            bookmaker_link = row.find('a', class_=lambda x: x and 'in-bookmaker-logo-link' in x)
            if bookmaker_link:
                bookmaker_name = bookmaker_link.get_text(strip=True)
            else:
                # Fallback - hledej v span
                bookmaker_span = row.find('span', class_=lambda x: x and 'in-bookmaker-logo' in x)
                if bookmaker_span:
                    bookmaker_name = bookmaker_span.get('title', 'Unknown')
                else:
                    logger.warning("Bookmaker n√°zev nenalezen v ≈ô√°dku")
                    return None
            
            # Najdi bu≈àky s kurzy (td elementy s data-odd atributem)
            odds_cells = row.find_all('td', {'data-odd': True})
            
            if len(odds_cells) < expected_odds_count:
                logger.warning(f"Nedostatek kurz≈Ø pro {bookmaker_name} (nalezeno {len(odds_cells)}, oƒçek√°v√°no {expected_odds_count})")
                return None
            
            # Extrahuj kurzy podle typu trhu
            odds_info = {}
            
            if expected_odds_count == 2:  # HA (Home/Away)
                odds_info['home_odd'] = float(odds_cells[0].get('data-odd'))
                odds_info['away_odd'] = float(odds_cells[1].get('data-odd'))
                
                # P≈ôidej opening odds pokud jsou dostupn√©
                for i, cell in enumerate(odds_cells[:2]):
                    prefix = 'home' if i == 0 else 'away'
                    self._add_additional_odds_info(cell, odds_info, prefix)
                    
            elif expected_odds_count == 3:  # 1X2 (Home/Draw/Away)
                odds_info['home_odd'] = float(odds_cells[0].get('data-odd'))
                odds_info['draw_odd'] = float(odds_cells[1].get('data-odd'))
                odds_info['away_odd'] = float(odds_cells[2].get('data-odd'))
                
                # P≈ôidej opening odds pokud jsou dostupn√©
                prefixes = ['home', 'draw', 'away']
                for i, cell in enumerate(odds_cells[:3]):
                    self._add_additional_odds_info(cell, odds_info, prefixes[i])
            
            return {
                'bookmaker': bookmaker_name,
                'market_type': market_name,
                'odds': odds_info,
                'timestamp': datetime.now(),
                'source': 'betexplorer_api'
            }

        except Exception as e:
            logger.error(f"Chyba p≈ôi parsov√°n√≠ ≈ô√°dku bookmaker: {e}")
            return None
    
    def _add_additional_odds_info(self, cell, odds_info: dict, prefix: str):
        """P≈ôid√° dodateƒçn√© informace o kurzech (opening odds, datumy)"""
        opening_odd = cell.get('data-opening-odd')
        opening_date = cell.get('data-opening-date')
        created_date = cell.get('data-created')
        
        if opening_odd:
            try:
                odds_info[f'{prefix}_opening_odd'] = float(opening_odd)
            except ValueError:
                pass
        
        if opening_date:
            odds_info[f'{prefix}_opening_date'] = opening_date
            
        if created_date:
            odds_info[f'{prefix}_created_date'] = created_date
    
    def _parse_average_odds(self, table, market_name: str, expected_odds_count: int) -> Optional[Dict]:
        """Parsuje pr≈Ømƒõrn√© kurzy z tfoot"""
        try:
            tfoot = table.find('tfoot')
            if not tfoot:
                return None
            
            # Najdi bu≈àky s pr≈Ømƒõrn√Ωmi kurzy
            avg_cells = tfoot.find_all('td', {'data-odd': True})
            
            if len(avg_cells) < expected_odds_count:
                return None
            
            # Extrahuj pr≈Ømƒõrn√© kurzy podle typu trhu
            odds_info = {}
            
            if expected_odds_count == 2:  # HA (Home/Away)
                odds_info['home_odd'] = float(avg_cells[0].get('data-odd'))
                odds_info['away_odd'] = float(avg_cells[1].get('data-odd'))
                
            elif expected_odds_count == 3:  # 1X2 (Home/Draw/Away)
                odds_info['home_odd'] = float(avg_cells[0].get('data-odd'))
                odds_info['draw_odd'] = float(avg_cells[1].get('data-odd'))
                odds_info['away_odd'] = float(avg_cells[2].get('data-odd'))
            
            return {
                'bookmaker': 'Average',
                'market_type': market_name,
                'odds': odds_info,
                'timestamp': datetime.now(),
                'source': 'betexplorer_api'
            }
            
        except Exception as e:
            logger.error(f"Chyba p≈ôi parsov√°n√≠ pr≈Ømƒõrn√Ωch kurz≈Ø: {e}")
            return None
    
    def scrape_season_odds(self, season: str, max_matches: Optional[int] = None) -> List[Dict]:
        """
        St√°hne kurzy pro celou sez√≥nu
        
        Args:
            season: Sez√≥na ve form√°tu '2021-2022'
            max_matches: Maximum z√°pas≈Ø ke sta≈æen√≠ (pro testov√°n√≠)
        
        Returns:
            Seznam slovn√≠k≈Ø s kurzy a informacemi o z√°pasech
        """
        logger.info(f"üèí Zaƒç√≠n√°m stahov√°n√≠ kurz≈Ø pro sez√≥nu {season}")
        
        # Get URLs of all matches
        match_urls = self.get_season_results_urls(season)
        if not match_urls:
            logger.error(f"Nenalezeny ≈æ√°dn√© z√°pasy pro sez√≥nu {season}")
            return []
        
        if max_matches:
            match_urls = match_urls[:max_matches]
            logger.info(f"Omezuji na prvn√≠ch {max_matches} z√°pas≈Ø")
        
        # Download odds for each match
        season_odds = []
        success_count = 0
        
        for i, match_url in enumerate(match_urls, 1):
            logger.info(f"Zpracov√°v√°m z√°pas {i}/{len(match_urls)}: {match_url}")
            
            try:
                match_odds = self.extract_match_odds(match_url)
                if match_odds:
                    match_odds['season'] = season
                    season_odds.append(match_odds)
                    success_count += 1
                    
                    if success_count % 10 == 0:
                        logger.info(f"‚úÖ √öspƒõ≈°nƒõ zpracov√°no {success_count} z√°pas≈Ø")
                
            except Exception as e:
                logger.error(f"Chyba p≈ôi zpracov√°n√≠ z√°pasu {match_url}: {e}")
                continue
        
        logger.info(f"üéâ Dokonƒçeno! Zpracov√°no {success_count}/{len(match_urls)} z√°pas≈Ø pro sez√≥nu {season}")
        return season_odds
    
    def scrape_multiple_seasons(self, seasons: List[str], max_matches_per_season: Optional[int] = None) -> Dict[str, List[Dict]]:
        """
        St√°hne kurzy pro v√≠ce sez√≥n
        
        Args:
            seasons: Seznam sez√≥n ve form√°tu ['2021-2022', '2022-2023']
            max_matches_per_season: Maximum z√°pas≈Ø na sez√≥nu
        
        Returns:
            Slovn√≠k s kurzy pro ka≈ædou sez√≥nu
        """
        all_seasons_data = {}
        
        for season in seasons:
            logger.info(f"\n{'='*50}")
            logger.info(f"STAHOV√ÅN√ç SEZ√ìNY {season}")
            logger.info(f"{'='*50}")
            
            try:
                season_odds = self.scrape_season_odds(season, max_matches_per_season)
                all_seasons_data[season] = season_odds
                
                # Save running results
                self.save_season_data(season, season_odds)
                
            except Exception as e:
                logger.error(f"Chyba p≈ôi stahov√°n√≠ sez√≥ny {season}: {e}")
                all_seasons_data[season] = []
        
        return all_seasons_data
    
    def save_season_data(self, season: str, season_data: List[Dict], 
                        output_dir: str = "data/odds") -> str:
        """
        Ulo≈æ√≠ data sez√≥ny do CSV souboru
        
        Args:
            season: N√°zev sez√≥ny
            season_data: Data sez√≥ny
            output_dir: V√Ωstupn√≠ adres√°≈ô
        
        Returns:
            Cesta k ulo≈æen√©mu souboru
        """
        os.makedirs(output_dir, exist_ok=True)
        
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        filename = f"nhl_odds_{season}_{timestamp}.csv"
        filepath = os.path.join(output_dir, filename)
        
        if not season_data:
            logger.warning(f"≈Ω√°dn√° data k ulo≈æen√≠ pro sez√≥nu {season}")
            return ""
        
        # P≈ôeveƒè data do DataFrame
        flattened_data = []
        
        for match in season_data:
            base_match_info = {
                'season': match.get('season'),
                'match_date': match.get('match_date'),
                'home_team': match.get('home_team'),
                'away_team': match.get('away_team'),
                'home_score': match.get('home_score'),
                'away_score': match.get('away_score'),
                'status': match.get('status'),
                'source_url': match.get('source_url'),
                'scraped_at': match.get('scraped_at')
            }
            
            # Expand odds
            if match.get('odds'):
                for odds_entry in match['odds']:
                    row = {**base_match_info}
                    row['bookmaker'] = odds_entry.get('bookmaker')
                    row['market_type'] = odds_entry.get('market_type')
                    
                    # Add odds
                    odds_dict = odds_entry.get('odds', {})
                    for market, odd_value in odds_dict.items():
                        row[f'odds_{market}'] = odd_value
                    
                    flattened_data.append(row)
            else:
                # No odds match
                flattened_data.append(base_match_info)
        
        # Save to CSV
        df = pd.DataFrame(flattened_data)
        df.to_csv(filepath, index=False, encoding='utf-8')
        
        logger.info(f"üíæ Data ulo≈æena do {filepath} ({len(df)} ≈ô√°dk≈Ø)")
        return filepath
    
    def close(self):
        """Ukonƒç√≠ scraper a uzav≈ôe WebDriver"""
        if self.driver:
            self.driver.quit()
            logger.info("üîí WebDriver uzav≈ôen")

def main():
    """Hlavn√≠ funkce pro spu≈°tƒõn√≠ scraperu"""
    
    # Create the necessary directories
    os.makedirs('data/odds', exist_ok=True)
    os.makedirs('logs', exist_ok=True)
    
    # Configuration
    SEASONS = ['2021-2022', '2022-2023', '2023-2024']  # Seasons to download
    MAX_MATCHES_PER_SEASON = 50  # For testing (50) - set to None for all matches
    USE_SELENIUM = True  # Use Selenium for dynamic content
    
    logger.info("üèí Spou≈°t√≠m BetExplorer scraper pro NHL kurzy")
    logger.info(f"Sez√≥ny: {', '.join(SEASONS)}")
    logger.info(f"Max z√°pas≈Ø na sez√≥nu: {MAX_MATCHES_PER_SEASON or 'v≈°echny'}")
    logger.info("Typy kurz≈Ø: HA (2-way Moneyline)") # , 1X2 (3-way Moneyline)")
    
    # Initialize scraper
    scraper = BetExplorerScraper(use_selenium=USE_SELENIUM)
    
    try:
        # Download data for all seasons
        all_data = scraper.scrape_multiple_seasons(
            seasons=SEASONS,
            max_matches_per_season=MAX_MATCHES_PER_SEASON
        )
        
        # Results summary
        logger.info("\n" + "="*60)
        logger.info("üìä SOUHRN STAHOV√ÅN√ç")
        logger.info("="*60)
        
        total_matches = 0
        for season, season_data in all_data.items():
            count = len(season_data)
            total_matches += count
            logger.info(f"  {season}: {count} z√°pas≈Ø")
        
        logger.info(f"  CELKEM: {total_matches} z√°pas≈Ø")
        
        # Save combined data
        if total_matches > 0:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            combined_file = f"data/odds/nhl_odds_combined_{timestamp}.json"
            
            with open(combined_file, 'w', encoding='utf-8') as f:
                json.dump(all_data, f, ensure_ascii=False, indent=2, default=str)
            
            logger.info(f"üíæ Kombinovan√° data ulo≈æena do {combined_file}")
        
        logger.info("üéâ Stahov√°n√≠ dokonƒçeno √∫spƒõ≈°nƒõ!")
        
    except KeyboardInterrupt:
        logger.info("‚èπÔ∏è Stahov√°n√≠ p≈ôeru≈°eno u≈æivatelem")
    except Exception as e:
        logger.error(f"‚ùå Chyba p≈ôi stahov√°n√≠: {e}")
        raise
    finally:
        # Close scraper
        scraper.close()

if __name__ == "__main__":
    main()
