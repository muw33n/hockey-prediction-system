{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üéØ Hockey Prediction System - Strategy Optimization Analysis\n",
    "\n",
    "**Deep dive into parameter optimization and strategy comparison**\n",
    "\n",
    "---\n",
    "\n",
    "## üìä Overview\n",
    "\n",
    "This notebook provides comprehensive strategy optimization analysis:\n",
    "- ‚úÖ **Parameter Sensitivity Analysis** - ROI impact of each parameter\n",
    "- üî• **Interactive Heatmaps** - Multi-dimensional parameter relationships\n",
    "- üèÜ **Strategy Comparison** - Head-to-head performance analysis\n",
    "- ‚ö° **A/B Testing Framework** - Statistical significance testing\n",
    "- üéØ **Optimization Insights** - Actionable parameter recommendations\n",
    "\n",
    "**Location:** `notebooks/analysis/strategy_optimization.ipynb`\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Setup & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import glob\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Visualization\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.figure_factory as ff\n",
    "\n",
    "# Statistics and analysis\n",
    "from scipy import stats\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from itertools import combinations\n",
    "\n",
    "# Configuration\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', 50)\n",
    "\n",
    "# Plotly theme\n",
    "import plotly.io as pio\n",
    "pio.templates.default = \"plotly_white\"\n",
    "\n",
    "print(\"üìö Libraries imported successfully!\")\n",
    "print(f\"üìÅ Working directory: {os.getcwd()}\")\n",
    "print(f\"üìÖ Analysis timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìÇ Enhanced Data Discovery & Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "RESULTS_DIR = Path(\"../../models/experiments\")\n",
    "CHARTS_EXPORT_DIR = Path(\"../../models/experiments/charts\")\n",
    "CHARTS_EXPORT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"üìÅ Results directory: {RESULTS_DIR.absolute()}\")\n",
    "print(f\"üìä Charts export directory: {CHARTS_EXPORT_DIR.absolute()}\")\n",
    "\n",
    "# Enhanced optimization file discovery\n",
    "def discover_optimization_files():\n",
    "    \"\"\"Discover and categorize optimization result files\"\"\"\n",
    "    \n",
    "    files = {\n",
    "        'detailed_csv': [],\n",
    "        'best_csv': [],\n",
    "        'results_json': [],\n",
    "        'analysis_json': []\n",
    "    }\n",
    "    \n",
    "    patterns = {\n",
    "        'detailed_csv': ['*optimization*detailed*.csv', '*parameter*detailed*.csv'],\n",
    "        'best_csv': ['*optimization*best*.csv', '*parameter*best*.csv'], \n",
    "        'results_json': ['*optimization*results*.json', '*parameter*results*.json'],\n",
    "        'analysis_json': ['*performance_analysis*.json']\n",
    "    }\n",
    "    \n",
    "    for file_type, pattern_list in patterns.items():\n",
    "        for pattern in pattern_list:\n",
    "            found_files = list(RESULTS_DIR.glob(pattern))\n",
    "            files[file_type].extend(found_files)\n",
    "        \n",
    "        # Remove duplicates and sort by modification time\n",
    "        files[file_type] = list(set(files[file_type]))\n",
    "        files[file_type] = sorted(files[file_type], key=lambda x: x.stat().st_mtime, reverse=True)\n",
    "    \n",
    "    return files\n",
    "\n",
    "# Safe loading functions\n",
    "def safe_load_optimization_csv(file_path):\n",
    "    \"\"\"Safely load optimization CSV with enhanced error handling\"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        # Convert numeric columns\n",
    "        numeric_cols = ['roi', 'max_drawdown', 'sharpe_ratio', 'total_bets', 'win_rate', \n",
    "                       'edge_threshold', 'min_odds', 'stake_size']\n",
    "        \n",
    "        for col in numeric_cols:\n",
    "            if col in df.columns:\n",
    "                df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "        \n",
    "        # Clean invalid data\n",
    "        df = df.dropna(subset=['roi'])  # ROI is essential\n",
    "        \n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error loading {file_path.name}: {e}\")\n",
    "        return None\n",
    "\n",
    "def safe_load_json(file_path):\n",
    "    \"\"\"Safely load JSON with error handling\"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            return json.load(f)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error loading {file_path.name}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Discover optimization files\n",
    "optimization_files = discover_optimization_files()\n",
    "\n",
    "print(\"\\nüîç Available optimization files:\")\n",
    "for file_type, file_list in optimization_files.items():\n",
    "    print(f\"  üìÑ {file_type}: {len(file_list)} files\")\n",
    "    for i, file_path in enumerate(file_list[:2]):\n",
    "        size_mb = file_path.stat().st_size / (1024*1024)\n",
    "        mod_time = datetime.fromtimestamp(file_path.stat().st_mtime)\n",
    "        print(f\"    [{i+1}] {file_path.name} ({size_mb:.1f}MB, {mod_time.strftime('%Y-%m-%d %H:%M')})\")\n",
    "    if len(file_list) > 2:\n",
    "        print(f\"    ... and {len(file_list) - 2} more\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced optimization data loading with intelligent source selection\n",
    "def load_optimization_data():\n",
    "    \"\"\"Load optimization data from best available source\"\"\"\n",
    "    \n",
    "    print(\"üìä Loading optimization data with priority: CSV detailed > JSON results\")\n",
    "    \n",
    "    # Priority 1: CSV detailed (fastest and most complete)\n",
    "    if optimization_files['detailed_csv']:\n",
    "        csv_path = optimization_files['detailed_csv'][0]\n",
    "        print(f\"üéØ Loading CSV detailed data: {csv_path.name}\")\n",
    "        \n",
    "        df = safe_load_optimization_csv(csv_path)\n",
    "        if df is not None and len(df) > 0:\n",
    "            print(f\"‚úÖ Loaded {len(df)} optimization records from CSV detailed\")\n",
    "            return df, \"CSV_detailed\", csv_path.name\n",
    "    \n",
    "    # Priority 2: JSON results (fallback)\n",
    "    if optimization_files['results_json']:\n",
    "        json_path = optimization_files['results_json'][0]\n",
    "        print(f\"üîÑ Fallback to JSON results: {json_path.name}\")\n",
    "        \n",
    "        data = safe_load_json(json_path)\n",
    "        if data and data.get('optimization_results'):\n",
    "            try:\n",
    "                # Convert optimization results to DataFrame\n",
    "                opt_records = []\n",
    "                for result in data['optimization_results']:\n",
    "                    if 'error' not in result and result.get('performance', {}).get('roi') is not None:\n",
    "                        record = {\n",
    "                            **result.get('parameters', {}),\n",
    "                            **result.get('performance', {}),\n",
    "                            **result.get('statistics', {})\n",
    "                        }\n",
    "                        opt_records.append(record)\n",
    "                \n",
    "                if opt_records:\n",
    "                    df = pd.DataFrame(opt_records)\n",
    "                    \n",
    "                    # Convert numeric columns\n",
    "                    numeric_cols = ['roi', 'max_drawdown', 'sharpe_ratio', 'total_bets', 'win_rate',\n",
    "                                   'edge_threshold', 'min_odds', 'stake_size']\n",
    "                    for col in numeric_cols:\n",
    "                        if col in df.columns:\n",
    "                            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "                    \n",
    "                    print(f\"‚úÖ Loaded {len(df)} optimization records from JSON\")\n",
    "                    return df, \"JSON_converted\", json_path.name\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Error converting JSON to DataFrame: {e}\")\n",
    "    \n",
    "    print(\"‚ùå No valid optimization data available\")\n",
    "    return None, None, None\n",
    "\n",
    "# Load optimization data\n",
    "print(\"\\nüì• Loading optimization data...\")\n",
    "opt_df, opt_source, opt_filename = load_optimization_data()\n",
    "\n",
    "# Data quality assessment\n",
    "if opt_df is not None:\n",
    "    print(\"\\nüìä Optimization Data Quality Assessment:\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    print(f\"‚úÖ Source: {opt_source}\")\n",
    "    print(f\"‚úÖ File: {opt_filename}\")\n",
    "    print(f\"‚úÖ Records loaded: {len(opt_df):,}\")\n",
    "    print(f\"‚úÖ Columns available: {len(opt_df.columns)}\")\n",
    "    \n",
    "    # Check key columns\n",
    "    key_columns = ['roi', 'edge_threshold', 'min_odds', 'stake_method', 'ev_method']\n",
    "    missing_cols = [col for col in key_columns if col not in opt_df.columns]\n",
    "    present_cols = [col for col in key_columns if col in opt_df.columns]\n",
    "    \n",
    "    print(f\"‚úÖ Key columns present: {', '.join(present_cols)}\")\n",
    "    if missing_cols:\n",
    "        print(f\"‚ö†Ô∏è Missing columns: {', '.join(missing_cols)}\")\n",
    "    \n",
    "    # Data completeness\n",
    "    if 'roi' in opt_df.columns:\n",
    "        valid_roi = opt_df['roi'].notna().sum()\n",
    "        roi_completeness = valid_roi / len(opt_df)\n",
    "        print(f\"‚úÖ ROI data completeness: {roi_completeness:.1%} ({valid_roi:,}/{len(opt_df):,})\")\n",
    "        \n",
    "        # ROI distribution summary\n",
    "        roi_stats = opt_df['roi'].describe()\n",
    "        profitable_pct = (opt_df['roi'] > 0).mean()\n",
    "        print(f\"‚úÖ Profitable strategies: {profitable_pct:.1%}\")\n",
    "        print(f\"‚úÖ ROI range: {roi_stats['min']:.2%} to {roi_stats['max']:.2%}\")\n",
    "        print(f\"‚úÖ Mean ROI: {roi_stats['mean']:.2%}\")\n",
    "    \n",
    "    print(f\"\\nüéØ Ready for strategy optimization analysis!\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå Cannot proceed with analysis - no optimization data available\")\n",
    "    print(\"\\nPlease ensure you have optimization results in one of these formats:\")\n",
    "    print(\"  - CSV: optimization_*_detailed_*.csv\")\n",
    "    print(\"  - JSON: optimization_*_results_*.json\")\n",
    "    print(f\"  - Location: {RESULTS_DIR.absolute()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Parameter Sensitivity Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive parameter sensitivity analysis\n",
    "if opt_df is not None and 'roi' in opt_df.columns:\n",
    "    \n",
    "    print(\"üéØ Parameter Sensitivity Analysis\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Detect available parameters for analysis\n",
    "    numeric_params = []\n",
    "    categorical_params = []\n",
    "    \n",
    "    # Standard parameter mappings\n",
    "    param_candidates = {\n",
    "        'edge_threshold': ['edge_threshold', 'edge_thresh', 'threshold'],\n",
    "        'min_odds': ['min_odds', 'minimum_odds', 'odds_min'],\n",
    "        'stake_size': ['stake_size', 'stake', 'bet_size'],\n",
    "        'stake_method': ['stake_method', 'staking_method', 'stake_type'],\n",
    "        'ev_method': ['ev_method', 'ev_calculation', 'expected_value_method']\n",
    "    }\n",
    "    \n",
    "    # Find available parameters\n",
    "    available_params = {}\n",
    "    for param_name, possible_names in param_candidates.items():\n",
    "        for col_name in possible_names:\n",
    "            if col_name in opt_df.columns:\n",
    "                available_params[param_name] = col_name\n",
    "                if opt_df[col_name].dtype == 'object':\n",
    "                    categorical_params.append(param_name)\n",
    "                else:\n",
    "                    numeric_params.append(param_name)\n",
    "                break\n",
    "    \n",
    "    print(f\"üìä Found parameters:\")\n",
    "    print(f\"   Numeric: {', '.join(numeric_params)}\")\n",
    "    print(f\"   Categorical: {', '.join(categorical_params)}\")\n",
    "    \n",
    "    if len(available_params) == 0:\n",
    "        print(\"‚ö†Ô∏è No standard parameters found. Available columns:\")\n",
    "        print(f\"   {', '.join(opt_df.columns[:15])}...\")\n",
    "    else:\n",
    "        # Correlation analysis for numeric parameters\n",
    "        correlations = {}\n",
    "        \n",
    "        for param_name in numeric_params:\n",
    "            col_name = available_params[param_name]\n",
    "            param_data = opt_df[[col_name, 'roi']].dropna()\n",
    "            \n",
    "            if len(param_data) > 10:  # Minimum data for meaningful correlation\n",
    "                try:\n",
    "                    # Pearson correlation\n",
    "                    pearson_r, pearson_p = pearsonr(param_data[col_name], param_data['roi'])\n",
    "                    \n",
    "                    # Spearman correlation (rank-based, more robust)\n",
    "                    spearman_r, spearman_p = spearmanr(param_data[col_name], param_data['roi'])\n",
    "                    \n",
    "                    correlations[param_name] = {\n",
    "                        'pearson': {'correlation': pearson_r, 'p_value': pearson_p},\n",
    "                        'spearman': {'correlation': spearman_r, 'p_value': spearman_p},\n",
    "                        'data_points': len(param_data),\n",
    "                        'column_name': col_name\n",
    "                    }\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"‚ö†Ô∏è Error calculating correlation for {param_name}: {e}\")\n",
    "        \n",
    "        # Display correlation results\n",
    "        if correlations:\n",
    "            print(f\"\\nüìà Parameter-ROI Correlations:\")\n",
    "            print(\"-\" * 70)\n",
    "            print(f\"{'Parameter':<15} {'Pearson':<10} {'P-value':<10} {'Spearman':<10} {'P-value':<10} {'N':<8}\")\n",
    "            print(\"-\" * 70)\n",
    "            \n",
    "            # Sort by absolute Pearson correlation\n",
    "            sorted_correlations = sorted(correlations.items(), \n",
    "                                       key=lambda x: abs(x[1]['pearson']['correlation']), \n",
    "                                       reverse=True)\n",
    "            \n",
    "            for param_name, corr_data in sorted_correlations:\n",
    "                pearson_r = corr_data['pearson']['correlation']\n",
    "                pearson_p = corr_data['pearson']['p_value']\n",
    "                spearman_r = corr_data['spearman']['correlation']\n",
    "                spearman_p = corr_data['spearman']['p_value']\n",
    "                n_points = corr_data['data_points']\n",
    "                \n",
    "                print(f\"{param_name:<15} {pearson_r:<10.3f} {pearson_p:<10.3f} {spearman_r:<10.3f} {spearman_p:<10.3f} {n_points:<8}\")\n",
    "                \n",
    "                # Interpretation\n",
    "                if abs(pearson_r) > 0.3 and pearson_p < 0.05:\n",
    "                    direction = \"positively\" if pearson_r > 0 else \"negatively\"\n",
    "                    strength = \"strongly\" if abs(pearson_r) > 0.5 else \"moderately\"\n",
    "                    print(f\"   ‚úÖ {param_name} is {strength} {direction} correlated with ROI\")\n",
    "                elif abs(pearson_r) > 0.1 and pearson_p < 0.1:\n",
    "                    direction = \"positively\" if pearson_r > 0 else \"negatively\"\n",
    "                    print(f\"   üìä {param_name} shows weak {direction} correlation with ROI\")\n",
    "                else:\n",
    "                    print(f\"   ‚ö™ {param_name} shows no significant correlation with ROI\")\n",
    "        \n",
    "        # Categorical parameter analysis\n",
    "        if categorical_params:\n",
    "            print(f\"\\nüìä Categorical Parameter Analysis:\")\n",
    "            \n",
    "            for param_name in categorical_params:\n",
    "                col_name = available_params[param_name]\n",
    "                param_analysis = opt_df.groupby(col_name)['roi'].agg([\n",
    "                    'count', 'mean', 'std', 'min', 'max'\n",
    "                ]).round(4)\n",
    "                \n",
    "                print(f\"\\nüîç {param_name} ({col_name}):\")\n",
    "                print(param_analysis.to_string())\n",
    "                \n",
    "                # Find best performing category\n",
    "                best_category = param_analysis['mean'].idxmax()\n",
    "                best_roi = param_analysis.loc[best_category, 'mean']\n",
    "                best_count = param_analysis.loc[best_category, 'count']\n",
    "                \n",
    "                print(f\"   üèÜ Best performing: {best_category} (ROI: {best_roi:.2%}, N: {best_count})\")\n",
    "                \n",
    "                # Statistical test (ANOVA) if multiple categories\n",
    "                if len(param_analysis) > 1:\n",
    "                    try:\n",
    "                        groups = [group['roi'].values for name, group in opt_df.groupby(col_name) if len(group) > 2]\n",
    "                        if len(groups) > 1:\n",
    "                            f_stat, p_value = stats.f_oneway(*groups)\n",
    "                            print(f\"   üìà ANOVA F-statistic: {f_stat:.3f}, p-value: {p_value:.3f}\")\n",
    "                            if p_value < 0.05:\n",
    "                                print(f\"   ‚úÖ Significant difference between {param_name} categories\")\n",
    "                            else:\n",
    "                                print(f\"   ‚ö™ No significant difference between {param_name} categories\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"   ‚ö†Ô∏è ANOVA test failed: {e}\")\n",
    "else:\n",
    "    print(\"‚ùå Cannot perform parameter sensitivity analysis - no optimization data or ROI column\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üî• Interactive Parameter Heatmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create interactive parameter heatmaps\n",
    "if opt_df is not None and len(numeric_params) >= 2:\n",
    "    \n",
    "    print(\"üî• Creating Parameter Heatmaps\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Create heatmaps for all pairs of numeric parameters\n",
    "    param_pairs = list(combinations(numeric_params, 2))\n",
    "    \n",
    "    print(f\"üìä Generating {len(param_pairs)} parameter combination heatmaps...\")\n",
    "    \n",
    "    for i, (param1, param2) in enumerate(param_pairs, 1):\n",
    "        \n",
    "        col1 = available_params[param1]\n",
    "        col2 = available_params[param2]\n",
    "        \n",
    "        print(f\"\\nüìà [{i}/{len(param_pairs)}] Creating heatmap: {param1} vs {param2}\")\n",
    "        \n",
    "        try:\n",
    "            # Prepare data\n",
    "            heatmap_data = opt_df[[col1, col2, 'roi']].dropna()\n",
    "            \n",
    "            if len(heatmap_data) < 10:\n",
    "                print(f\"   ‚ö†Ô∏è Insufficient data for {param1} vs {param2} heatmap (only {len(heatmap_data)} points)\")\n",
    "                continue\n",
    "            \n",
    "            # Create bins for both parameters\n",
    "            n_bins = min(10, int(np.sqrt(len(heatmap_data))))\n",
    "            \n",
    "            # Bin the data\n",
    "            try:\n",
    "                heatmap_data[f'{param1}_bin'] = pd.cut(heatmap_data[col1], bins=n_bins, precision=3)\n",
    "                heatmap_data[f'{param2}_bin'] = pd.cut(heatmap_data[col2], bins=n_bins, precision=3)\n",
    "            except ValueError:\n",
    "                # Fallback to quantile-based binning\n",
    "                heatmap_data[f'{param1}_bin'] = pd.qcut(heatmap_data[col1], q=n_bins, duplicates='drop', precision=3)\n",
    "                heatmap_data[f'{param2}_bin'] = pd.qcut(heatmap_data[col2], q=n_bins, duplicates='drop', precision=3)\n",
    "            \n",
    "            # Create pivot table\n",
    "            pivot_table = heatmap_data.groupby([f'{param1}_bin', f'{param2}_bin'])['roi'].agg([\n",
    "                'mean', 'count'\n",
    "            ]).reset_index()\n",
    "            \n",
    "            # Filter out bins with too few data points\n",
    "            pivot_table = pivot_table[pivot_table['count'] >= 3]\n",
    "            \n",
    "            if len(pivot_table) == 0:\n",
    "                print(f\"   ‚ö†Ô∏è No sufficient data bins for {param1} vs {param2} heatmap\")\n",
    "                continue\n",
    "            \n",
    "            # Create pivot for heatmap\n",
    "            pivot_for_heatmap = pivot_table.pivot(index=f'{param1}_bin', \n",
    "                                                 columns=f'{param2}_bin', \n",
    "                                                 values='mean')\n",
    "            \n",
    "            # Create interactive heatmap\n",
    "            fig_heatmap = px.imshow(\n",
    "                pivot_for_heatmap.values,\n",
    "                x=[str(col) for col in pivot_for_heatmap.columns],\n",
    "                y=[str(idx) for idx in pivot_for_heatmap.index],\n",
    "                color_continuous_scale='RdYlGn',\n",
    "                aspect='auto',\n",
    "                title=f\"üî• ROI Heatmap: {param1.replace('_', ' ').title()} vs {param2.replace('_', ' ').title()}\",\n",
    "                labels={'color': 'ROI', 'x': param2.replace('_', ' ').title(), 'y': param1.replace('_', ' ').title()}\n",
    "            )\n",
    "            \n",
    "            # Add text annotations for ROI values\n",
    "            for i in range(len(pivot_for_heatmap.index)):\n",
    "                for j in range(len(pivot_for_heatmap.columns)):\n",
    "                    value = pivot_for_heatmap.iloc[i, j]\n",
    "                    if not pd.isna(value):\n",
    "                        fig_heatmap.add_annotation(\n",
    "                            x=j, y=i,\n",
    "                            text=f\"{value:.1%}\",\n",
    "                            showarrow=False,\n",
    "                            font=dict(color=\"white\" if abs(value) > pivot_for_heatmap.abs().quantile(0.7).max() else \"black\", size=10)\n",
    "                        )\n",
    "            \n",
    "            fig_heatmap.update_layout(\n",
    "                height=600,\n",
    "                xaxis_title=param2.replace('_', ' ').title(),\n",
    "                yaxis_title=param1.replace('_', ' ').title()\n",
    "            )\n",
    "            \n",
    "            fig_heatmap.show()\n",
    "            \n",
    "            # Save heatmap\n",
    "            heatmap_filename = f\"heatmap_{param1}_vs_{param2}.html\"\n",
    "            fig_heatmap.write_html(CHARTS_EXPORT_DIR / heatmap_filename)\n",
    "            print(f\"   üíæ Heatmap saved: {heatmap_filename}\")\n",
    "            \n",
    "            # Find best performing combination\n",
    "            best_combo_idx = pivot_table['mean'].idxmax()\n",
    "            best_combo = pivot_table.loc[best_combo_idx]\n",
    "            \n",
    "            print(f\"   üèÜ Best combination:\")\n",
    "            print(f\"      {param1}: {best_combo[f'{param1}_bin']}\")\n",
    "            print(f\"      {param2}: {best_combo[f'{param2}_bin']}\")\n",
    "            print(f\"      ROI: {best_combo['mean']:.2%} (N: {best_combo['count']})\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ö†Ô∏è Error creating heatmap for {param1} vs {param2}: {e}\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ Parameter heatmap analysis completed!\")\n",
    "    \n",
    "elif opt_df is not None:\n",
    "    print(\"‚ö†Ô∏è Need at least 2 numeric parameters for heatmap analysis\")\n",
    "    print(f\"Available numeric parameters: {', '.join(numeric_params) if numeric_params else 'None'}\")\n",
    "else:\n",
    "    print(\"‚ùå Cannot create heatmaps - no optimization data available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèÜ Strategy Comparison & Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive strategy comparison\n",
    "if opt_df is not None and 'roi' in opt_df.columns:\n",
    "    \n",
    "    print(\"üèÜ Strategy Comparison & Ranking\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Define multiple ranking criteria\n",
    "    ranking_criteria = {\n",
    "        'roi': {'ascending': False, 'label': 'Highest ROI'},\n",
    "        'sharpe_ratio': {'ascending': False, 'label': 'Best Risk-Adjusted Return'},\n",
    "        'total_bets': {'ascending': False, 'label': 'Most Active Strategy'},\n",
    "        'win_rate': {'ascending': False, 'label': 'Highest Win Rate'}\n",
    "    }\n",
    "    \n",
    "    # Filter to only profitable strategies\n",
    "    profitable_strategies = opt_df[opt_df['roi'] > 0].copy()\n",
    "    \n",
    "    if len(profitable_strategies) == 0:\n",
    "        print(\"‚ö†Ô∏è No profitable strategies found - analyzing all strategies\")\n",
    "        strategy_pool = opt_df.copy()\n",
    "        pool_description = \"all strategies\"\n",
    "    else:\n",
    "        strategy_pool = profitable_strategies.copy()\n",
    "        pool_description = \"profitable strategies\"\n",
    "        print(f\"üìä Analyzing {len(strategy_pool)} profitable strategies out of {len(opt_df)} total\")\n",
    "    \n",
    "    # Create comprehensive ranking analysis\n",
    "    strategy_rankings = {}\n",
    "    \n",
    "    for metric, criteria in ranking_criteria.items():\n",
    "        if metric in strategy_pool.columns:\n",
    "            \n",
    "            # Clean data for this metric\n",
    "            metric_data = strategy_pool.dropna(subset=[metric])\n",
    "            \n",
    "            if len(metric_data) > 0:\n",
    "                top_strategies = metric_data.nlargest(10, metric) if not criteria['ascending'] else metric_data.nsmallest(10, metric)\n",
    "                \n",
    "                strategy_rankings[metric] = {\n",
    "                    'label': criteria['label'],\n",
    "                    'top_strategies': top_strategies,\n",
    "                    'metric_stats': metric_data[metric].describe(),\n",
    "                    'data_points': len(metric_data)\n",
    "                }\n",
    "                \n",
    "                print(f\"\\nüéØ {criteria['label']} (Top 5 {pool_description}):\")\n",
    "                print(\"-\" * 80)\n",
    "                \n",
    "                # Display top strategies\n",
    "                display_cols = [metric]\n",
    "                for col in ['roi', 'max_drawdown', 'total_bets', 'win_rate', 'edge_threshold', 'min_odds']:\n",
    "                    if col in top_strategies.columns and col != metric:\n",
    "                        display_cols.append(col)\n",
    "                \n",
    "                top_5 = top_strategies.head(5)[display_cols]\n",
    "                \n",
    "                # Format for display\n",
    "                display_df = top_5.copy()\n",
    "                for col in ['roi', 'win_rate', 'max_drawdown']:\n",
    "                    if col in display_df.columns:\n",
    "                        display_df[col] = display_df[col].apply(lambda x: f\"{x:.2%}\" if pd.notna(x) else \"N/A\")\n",
    "                \n",
    "                for col in ['edge_threshold', 'min_odds']:\n",
    "                    if col in display_df.columns:\n",
    "                        display_df[col] = display_df[col].apply(lambda x: f\"{x:.3f}\" if pd.notna(x) else \"N/A\")\n",
    "                \n",
    "                print(display_df.to_string(index=False))\n",
    "                \n",
    "                # Best strategy insight\n",
    "                best_strategy = top_strategies.iloc[0]\n",
    "                best_value = best_strategy[metric]\n",
    "                \n",
    "                if metric == 'roi':\n",
    "                    print(f\"   üèÜ Best {metric}: {best_value:.2%}\")\n",
    "                elif metric in ['win_rate', 'max_drawdown']:\n",
    "                    print(f\"   üèÜ Best {metric}: {best_value:.2%}\")\n",
    "                else:\n",
    "                    print(f\"   üèÜ Best {metric}: {best_value:.3f}\")\n",
    "                \n",
    "                # Strategy parameters\n",
    "                if 'edge_threshold' in best_strategy:\n",
    "                    print(f\"   üìä Parameters: edge={best_strategy.get('edge_threshold', 'N/A'):.3f}, odds={best_strategy.get('min_odds', 'N/A'):.2f}\")\n",
    "    \n",
    "    # Cross-ranking analysis\n",
    "    if len(strategy_rankings) > 1:\n",
    "        print(f\"\\nüîÑ Cross-Ranking Analysis:\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        # Find strategies that appear in multiple top 10 lists\n",
    "        all_top_strategies = set()\n",
    "        metric_appearances = {}\n",
    "        \n",
    "        for metric, ranking_data in strategy_rankings.items():\n",
    "            top_indices = set(ranking_data['top_strategies'].index)\n",
    "            all_top_strategies.update(top_indices)\n",
    "            \n",
    "            for idx in top_indices:\n",
    "                if idx not in metric_appearances:\n",
    "                    metric_appearances[idx] = []\n",
    "                metric_appearances[idx].append(metric)\n",
    "        \n",
    "        # Find multi-metric winners\n",
    "        multi_winners = {idx: metrics for idx, metrics in metric_appearances.items() if len(metrics) > 1}\n",
    "        \n",
    "        if multi_winners:\n",
    "            print(f\"üèÖ Strategies appearing in multiple top-10 lists:\")\n",
    "            \n",
    "            for idx, metrics in sorted(multi_winners.items(), key=lambda x: len(x[1]), reverse=True)[:5]:\n",
    "                strategy = strategy_pool.loc[idx]\n",
    "                metrics_str = ', '.join(metrics)\n",
    "                print(f\"   Strategy {idx}: Top-10 in {len(metrics)} metrics ({metrics_str})\")\n",
    "                print(f\"      ROI: {strategy.get('roi', 0):.2%}, Total Bets: {strategy.get('total_bets', 0)}\")\n",
    "        else:\n",
    "            print(f\"   No strategies appear in multiple top-10 lists\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå Cannot perform strategy comparison - no optimization data or ROI column\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚ö° A/B Testing Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A/B Testing Framework for strategy comparison\n",
    "if opt_df is not None and len(categorical_params) > 0:\n",
    "    \n",
    "    print(\"‚ö° A/B Testing Framework\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Test each categorical parameter\n",
    "    for param_name in categorical_params:\n",
    "        col_name = available_params[param_name]\n",
    "        \n",
    "        print(f\"\\nüß™ A/B Testing: {param_name} ({col_name})\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        # Get unique categories\n",
    "        categories = opt_df[col_name].value_counts()\n",
    "        \n",
    "        if len(categories) < 2:\n",
    "            print(f\"   ‚ö†Ô∏è Only one category found for {param_name}, skipping A/B test\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"   üìä Categories found: {', '.join([f'{cat}({count})' for cat, count in categories.items()])}\")\n",
    "        \n",
    "        # Perform pairwise comparisons for all category pairs\n",
    "        category_list = list(categories.index)\n",
    "        \n",
    "        for i, cat_a in enumerate(category_list):\n",
    "            for cat_b in category_list[i+1:]:\n",
    "                \n",
    "                # Get data for both categories\n",
    "                group_a = opt_df[opt_df[col_name] == cat_a]['roi'].dropna()\n",
    "                group_b = opt_df[opt_df[col_name] == cat_b]['roi'].dropna()\n",
    "                \n",
    "                if len(group_a) < 5 or len(group_b) < 5:\n",
    "                    print(f\"   ‚ö†Ô∏è Insufficient data for {cat_a} vs {cat_b} (need ‚â•5 samples each)\")\n",
    "                    continue\n",
    "                \n",
    "                print(f\"\\n   üî¨ Test: {cat_a} vs {cat_b}\")\n",
    "                \n",
    "                # Descriptive statistics\n",
    "                stats_a = {\n",
    "                    'mean': group_a.mean(),\n",
    "                    'std': group_a.std(),\n",
    "                    'median': group_a.median(),\n",
    "                    'count': len(group_a)\n",
    "                }\n",
    "                \n",
    "                stats_b = {\n",
    "                    'mean': group_b.mean(),\n",
    "                    'std': group_b.std(),\n",
    "                    'median': group_b.median(),\n",
    "                    'count': len(group_b)\n",
    "                }\n",
    "                \n",
    "                print(f\"      {cat_a:>15}: Œº={stats_a['mean']:.3%}, œÉ={stats_a['std']:.3%}, median={stats_a['median']:.3%}, n={stats_a['count']}\")\n",
    "                print(f\"      {cat_b:>15}: Œº={stats_b['mean']:.3%}, œÉ={stats_b['std']:.3%}, median={stats_b['median']:.3%}, n={stats_b['count']}\")\n",
    "                \n",
    "                # Effect size (Cohen's d)\n",
    "                pooled_std = np.sqrt(((stats_a['count']-1)*stats_a['std']**2 + (stats_b['count']-1)*stats_b['std']**2) / (stats_a['count'] + stats_b['count'] - 2))\n",
    "                cohens_d = (stats_a['mean'] - stats_b['mean']) / pooled_std if pooled_std > 0 else 0\n",
    "                \n",
    "                # Statistical tests\n",
    "                try:\n",
    "                    # T-test (parametric)\n",
    "                    t_stat, t_p_value = stats.ttest_ind(group_a, group_b)\n",
    "                    \n",
    "                    # Mann-Whitney U test (non-parametric)\n",
    "                    u_stat, u_p_value = stats.mannwhitneyu(group_a, group_b, alternative='two-sided')\n",
    "                    \n",
    "                    print(f\"      T-test: t={t_stat:.3f}, p={t_p_value:.4f}\")\n",
    "                    print(f\"      Mann-Whitney U: U={u_stat:.1f}, p={u_p_value:.4f}\")\n",
    "                    print(f\"      Effect size (Cohen's d): {cohens_d:.3f}\")\n",
    "                    \n",
    "                    # Interpretation\n",
    "                    significant = min(t_p_value, u_p_value) < 0.05\n",
    "                    practical = abs(cohens_d) > 0.2  # Small effect size threshold\n",
    "                    \n",
    "                    if significant and practical:\n",
    "                        winner = cat_a if stats_a['mean'] > stats_b['mean'] else cat_b\n",
    "                        diff_pct = abs(stats_a['mean'] - stats_b['mean'])\n",
    "                        print(f\"      ‚úÖ SIGNIFICANT DIFFERENCE: {winner} is better by {diff_pct:.2%}\")\n",
    "                        \n",
    "                        # Effect size interpretation\n",
    "                        if abs(cohens_d) < 0.5:\n",
    "                            print(f\"         Small effect size\")\n",
    "                        elif abs(cohens_d) < 0.8:\n",
    "                            print(f\"         Medium effect size\")\n",
    "                        else:\n",
    "                            print(f\"         Large effect size\")\n",
    "                            \n",
    "                    elif significant:\n",
    "                        print(f\"      üìä Statistically significant but small practical effect\")\n",
    "                    elif practical:\n",
    "                        print(f\"      üìä Practical difference but not statistically significant (need more data)\")\n",
    "                    else:\n",
    "                        print(f\"      ‚ö™ No significant difference between {cat_a} and {cat_b}\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"      ‚ö†Ô∏è Statistical test failed: {e}\")\n",
    "        \n",
    "        # Overall recommendation for this parameter\n",
    "        print(f\"\\n   üí° {param_name} Recommendation:\")\n",
    "        overall_analysis = opt_df.groupby(col_name)['roi'].agg(['mean', 'count', 'std']).sort_values('mean', ascending=False)\n",
    "        \n",
    "        if len(overall_analysis) > 0:\n",
    "            best_category = overall_analysis.index[0]\n",
    "            best_roi = overall_analysis.loc[best_category, 'mean']\n",
    "            best_count = overall_analysis.loc[best_category, 'count']\n",
    "            \n",
    "            print(f\"      üèÜ Recommended: {best_category}\")\n",
    "            print(f\"      üìä Average ROI: {best_roi:.2%} (based on {best_count} strategies)\")\n",
    "            \n",
    "            # Confidence assessment\n",
    "            if best_count >= 20:\n",
    "                print(f\"      ‚úÖ High confidence (‚â•20 data points)\")\n",
    "            elif best_count >= 10:\n",
    "                print(f\"      üìä Medium confidence (10-19 data points)\")\n",
    "            else:\n",
    "                print(f\"      ‚ö†Ô∏è Low confidence (<10 data points)\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ A/B Testing framework analysis completed!\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå Cannot perform A/B testing - no optimization data or categorical parameters\")\n",
    "    if opt_df is not None:\n",
    "        print(f\"Available categorical parameters: {', '.join(categorical_params) if categorical_params else 'None'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Optimization Insights & Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive optimization insights and actionable recommendations\n",
    "if opt_df is not None:\n",
    "    \n",
    "    print(\"üéØ OPTIMIZATION INSIGHTS & RECOMMENDATIONS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Collect key insights\n",
    "    insights = []\n",
    "    recommendations = []\n",
    "    \n",
    "    # 1. Overall Performance Insights\n",
    "    total_strategies = len(opt_df)\n",
    "    profitable_count = (opt_df['roi'] > 0).sum()\n",
    "    profitable_rate = profitable_count / total_strategies\n",
    "    \n",
    "    best_roi = opt_df['roi'].max()\n",
    "    worst_roi = opt_df['roi'].min()\n",
    "    mean_roi = opt_df['roi'].mean()\n",
    "    \n",
    "    print(f\"\\nüìä OVERALL PERFORMANCE SUMMARY:\")\n",
    "    print(f\"   Strategies tested: {total_strategies:,}\")\n",
    "    print(f\"   Profitable strategies: {profitable_count:,} ({profitable_rate:.1%})\")\n",
    "    print(f\"   ROI range: {worst_roi:.2%} to {best_roi:.2%}\")\n",
    "    print(f\"   Mean ROI: {mean_roi:.2%}\")\n",
    "    \n",
    "    if profitable_rate > 0.3:\n",
    "        insights.append(\"‚úÖ Strong optimization results: >30% of strategies are profitable\")\n",
    "        recommendations.append(\"Proceed with top strategies for live testing\")\n",
    "    elif profitable_rate > 0.1:\n",
    "        insights.append(\"üìä Moderate optimization results: 10-30% of strategies are profitable\")\n",
    "        recommendations.append(\"Focus on top 5% of strategies only\")\n",
    "    else:\n",
    "        insights.append(\"üî¥ Poor optimization results: <10% of strategies are profitable\")\n",
    "        recommendations.append(\"Re-evaluate model and parameter ranges\")\n",
    "    \n",
    "    # 2. Parameter-Specific Insights\n",
    "    if correlations:\n",
    "        print(f\"\\nüìà PARAMETER IMPACT ANALYSIS:\")\n",
    "        \n",
    "        # Most impactful parameters\n",
    "        impact_ranking = sorted(correlations.items(), \n",
    "                              key=lambda x: abs(x[1]['pearson']['correlation']), \n",
    "                              reverse=True)\n",
    "        \n",
    "        for i, (param, corr_data) in enumerate(impact_ranking[:3], 1):\n",
    "            correlation = corr_data['pearson']['correlation']\n",
    "            p_value = corr_data['pearson']['p_value']\n",
    "            \n",
    "            print(f\"   {i}. {param}: correlation = {correlation:.3f}, p = {p_value:.4f}\")\n",
    "            \n",
    "            if abs(correlation) > 0.3 and p_value < 0.05:\n",
    "                direction = \"increase\" if correlation > 0 else \"decrease\"\n",
    "                insights.append(f\"üéØ {param} strongly impacts ROI - {direction} to improve performance\")\n",
    "            elif abs(correlation) > 0.1:\n",
    "                insights.append(f\"üìä {param} has moderate impact on ROI\")\n",
    "    \n",
    "    # 3. Optimal Parameter Ranges\n",
    "    if len(profitable_strategies) > 0:\n",
    "        print(f\"\\nüéØ OPTIMAL PARAMETER RANGES (from profitable strategies):\")\n",
    "        \n",
    "        for param_name in numeric_params:\n",
    "            col_name = available_params[param_name]\n",
    "            \n",
    "            if col_name in profitable_strategies.columns:\n",
    "                param_data = profitable_strategies[col_name].dropna()\n",
    "                \n",
    "                if len(param_data) > 0:\n",
    "                    q25 = param_data.quantile(0.25)\n",
    "                    q75 = param_data.quantile(0.75)\n",
    "                    median = param_data.median()\n",
    "                    \n",
    "                    print(f\"   {param_name}: {q25:.3f} - {q75:.3f} (median: {median:.3f})\")\n",
    "                    recommendations.append(f\"Set {param_name} between {q25:.3f} and {q75:.3f}\")\n",
    "    \n",
    "    # 4. Strategy Method Recommendations\n",
    "    if categorical_params:\n",
    "        print(f\"\\nüîß STRATEGY METHOD RECOMMENDATIONS:\")\n",
    "        \n",
    "        for param_name in categorical_params:\n",
    "            col_name = available_params[param_name]\n",
    "            \n",
    "            method_performance = opt_df.groupby(col_name)['roi'].agg(['mean', 'count']).sort_values('mean', ascending=False)\n",
    "            \n",
    "            if len(method_performance) > 0:\n",
    "                best_method = method_performance.index[0]\n",
    "                best_roi = method_performance.loc[best_method, 'mean']\n",
    "                best_count = method_performance.loc[best_method, 'count']\n",
    "                \n",
    "                print(f\"   {param_name}: {best_method} (ROI: {best_roi:.2%}, N: {best_count})\")\n",
    "                \n",
    "                if best_count >= 10:\n",
    "                    recommendations.append(f\"Use {best_method} for {param_name}\")\n",
    "                else:\n",
    "                    recommendations.append(f\"Consider {best_method} for {param_name} (limited data)\")\n",
    "    \n",
    "    # 5. Risk Assessment\n",
    "    if 'max_drawdown' in opt_df.columns:\n",
    "        drawdown_data = opt_df['max_drawdown'].dropna()\n",
    "        \n",
    "        if len(drawdown_data) > 0:\n",
    "            avg_drawdown = drawdown_data.mean()\n",
    "            max_drawdown = drawdown_data.max()\n",
    "            \n",
    "            print(f\"\\n‚ö†Ô∏è RISK ASSESSMENT:\")\n",
    "            print(f\"   Average max drawdown: {avg_drawdown:.2%}\")\n",
    "            print(f\"   Worst max drawdown: {max_drawdown:.2%}\")\n",
    "            \n",
    "            if max_drawdown > 0.5:\n",
    "                insights.append(\"üî¥ High risk detected: some strategies have >50% drawdown\")\n",
    "                recommendations.append(\"Implement strict risk management (max 2% position size)\")\n",
    "            elif max_drawdown > 0.3:\n",
    "                insights.append(\"‚ö†Ô∏è Moderate risk: some strategies have >30% drawdown\")\n",
    "                recommendations.append(\"Use conservative position sizing (max 5% risk per bet)\")\n",
    "            else:\n",
    "                insights.append(\"‚úÖ Acceptable risk levels: drawdowns under control\")\n",
    "    \n",
    "    # 6. Implementation Priority\n",
    "    if len(profitable_strategies) > 0:\n",
    "        # Find strategies with best balance of ROI and sample size\n",
    "        profitable_strategies['score'] = profitable_strategies['roi'] * np.log(profitable_strategies.get('total_bets', 100))\n",
    "        top_implementation = profitable_strategies.nlargest(5, 'score')\n",
    "        \n",
    "        print(f\"\\nüöÄ TOP 5 STRATEGIES FOR IMPLEMENTATION:\")\n",
    "        for i, (idx, strategy) in enumerate(top_implementation.iterrows(), 1):\n",
    "            roi = strategy.get('roi', 0)\n",
    "            bets = strategy.get('total_bets', 0)\n",
    "            drawdown = strategy.get('max_drawdown', 0)\n",
    "            \n",
    "            print(f\"   {i}. ROI: {roi:.2%}, Bets: {bets}, Drawdown: {drawdown:.2%}\")\n",
    "    \n",
    "    # Summary of insights and recommendations\n",
    "    print(f\"\\nüí° KEY INSIGHTS:\")\n",
    "    for i, insight in enumerate(insights, 1):\n",
    "        print(f\"   {i}. {insight}\")\n",
    "    \n",
    "    print(f\"\\nüéØ ACTIONABLE RECOMMENDATIONS:\")\n",
    "    for i, recommendation in enumerate(recommendations, 1):\n",
    "        print(f\"   {i}. {recommendation}\")\n",
    "    \n",
    "    # Generate final assessment\n",
    "    print(f\"\\nüèÅ FINAL ASSESSMENT:\")\n",
    "    \n",
    "    if profitable_rate > 0.2 and best_roi > 0.1:\n",
    "        assessment = \"üü¢ PROCEED WITH CONFIDENCE\"\n",
    "        action = \"Ready for careful live implementation with top strategies\"\n",
    "    elif profitable_rate > 0.1 and best_roi > 0.05:\n",
    "        assessment = \"üü° PROCEED WITH CAUTION\"\n",
    "        action = \"Paper trade first, then implement with very conservative sizing\"\n",
    "    else:\n",
    "        assessment = \"üî¥ DO NOT IMPLEMENT YET\"\n",
    "        action = \"Further optimization and model improvement needed\"\n",
    "    \n",
    "    print(f\"   Status: {assessment}\")\n",
    "    print(f\"   Action: {action}\")\n",
    "    \n",
    "    print(f\"\\nüìÖ Analysis completed: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    print(f\"üìÅ Charts exported to: {CHARTS_EXPORT_DIR.absolute()}\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå Cannot generate insights - no optimization data available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì§ Export & Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create optimization summary dashboard\n",
    "def create_optimization_dashboard():\n",
    "    \"\"\"Create comprehensive optimization dashboard\"\"\"\n",
    "    \n",
    "    if opt_df is None:\n",
    "        print(\"‚ùå Cannot create dashboard - no optimization data\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        # Create 2x3 dashboard\n",
    "        fig = make_subplots(\n",
    "            rows=3, cols=2,\n",
    "            subplot_titles=('ROI Distribution', 'Parameter Correlations',\n",
    "                          'Top Strategies Performance', 'Risk vs Return',\n",
    "                          'Strategy Method Comparison', 'Optimization Summary'),\n",
    "            specs=[[{\"type\": \"histogram\"}, {\"type\": \"bar\"}],\n",
    "                   [{\"type\": \"scatter\"}, {\"type\": \"scatter\"}],\n",
    "                   [{\"type\": \"bar\"}, {\"type\": \"table\"}]]\n",
    "        )\n",
    "        \n",
    "        # 1. ROI Distribution\n",
    "        fig.add_trace(\n",
    "            go.Histogram(x=opt_df['roi'], name='ROI Distribution', nbinsx=30, showlegend=False),\n",
    "            row=1, col=1\n",
    "        )\n",
    "        \n",
    "        # 2. Parameter Correlations (if available)\n",
    "        if correlations:\n",
    "            params = list(correlations.keys())\n",
    "            corr_values = [correlations[p]['pearson']['correlation'] for p in params]\n",
    "            \n",
    "            fig.add_trace(\n",
    "                go.Bar(x=params, y=corr_values, name='ROI Correlation', showlegend=False),\n",
    "                row=1, col=2\n",
    "            )\n",
    "        \n",
    "        # 3. Top Strategies Performance\n",
    "        top_strategies = opt_df.nlargest(20, 'roi')\n",
    "        fig.add_trace(\n",
    "            go.Scatter(x=list(range(1, 21)), y=top_strategies['roi'], \n",
    "                     mode='markers+lines', name='Top 20 ROI', showlegend=False),\n",
    "            row=2, col=1\n",
    "        )\n",
    "        \n",
    "        # 4. Risk vs Return (if drawdown available)\n",
    "        if 'max_drawdown' in opt_df.columns:\n",
    "            risk_return_data = opt_df[opt_df['roi'] > 0].head(100)  # Top 100 profitable\n",
    "            fig.add_trace(\n",
    "                go.Scatter(x=risk_return_data['max_drawdown'], y=risk_return_data['roi'],\n",
    "                         mode='markers', name='Risk vs Return', showlegend=False),\n",
    "                row=2, col=2\n",
    "            )\n",
    "        \n",
    "        # 5. Strategy Method Comparison (if categorical params available)\n",
    "        if categorical_params and len(categorical_params) > 0:\n",
    "            param = categorical_params[0]\n",
    "            col_name = available_params[param]\n",
    "            method_perf = opt_df.groupby(col_name)['roi'].mean().sort_values(ascending=False)\n",
    "            \n",
    "            fig.add_trace(\n",
    "                go.Bar(x=list(method_perf.index), y=method_perf.values, \n",
    "                      name=f'{param} Performance', showlegend=False),\n",
    "                row=3, col=1\n",
    "            )\n",
    "        \n",
    "        # 6. Summary Table\n",
    "        summary_data = [\n",
    "            ['Total Strategies', f\"{len(opt_df):,}\"],\n",
    "            ['Profitable', f\"{(opt_df['roi'] > 0).sum():,}\"],\n",
    "            ['Profitability Rate', f\"{(opt_df['roi'] > 0).mean():.1%}\"],\n",
    "            ['Best ROI', f\"{opt_df['roi'].max():.2%}\"],\n",
    "            ['Mean ROI', f\"{opt_df['roi'].mean():.2%}\"],\n",
    "            ['Std ROI', f\"{opt_df['roi'].std():.2%}\"]\n",
    "        ]\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Table(\n",
    "                header=dict(values=['Metric', 'Value'], font=dict(size=12)),\n",
    "                cells=dict(values=[[row[0] for row in summary_data], \n",
    "                                  [row[1] for row in summary_data]], font=dict(size=10))\n",
    "            ),\n",
    "            row=3, col=2\n",
    "        )\n",
    "        \n",
    "        fig.update_layout(\n",
    "            height=1200,\n",
    "            title_text=\"üéØ Hockey Prediction System - Strategy Optimization Dashboard\",\n",
    "            showlegend=False\n",
    "        )\n",
    "        \n",
    "        # Update axis labels\n",
    "        fig.update_xaxes(title_text=\"ROI\", row=1, col=1)\n",
    "        fig.update_yaxes(title_text=\"Count\", row=1, col=1)\n",
    "        \n",
    "        if correlations:\n",
    "            fig.update_xaxes(title_text=\"Parameters\", row=1, col=2)\n",
    "            fig.update_yaxes(title_text=\"Correlation with ROI\", row=1, col=2)\n",
    "        \n",
    "        fig.update_xaxes(title_text=\"Strategy Rank\", row=2, col=1)\n",
    "        fig.update_yaxes(title_text=\"ROI\", row=2, col=1)\n",
    "        \n",
    "        if 'max_drawdown' in opt_df.columns:\n",
    "            fig.update_xaxes(title_text=\"Max Drawdown\", row=2, col=2)\n",
    "            fig.update_yaxes(title_text=\"ROI\", row=2, col=2)\n",
    "        \n",
    "        # Save dashboard\n",
    "        dashboard_path = CHARTS_EXPORT_DIR / \"strategy_optimization_dashboard.html\"\n",
    "        fig.write_html(dashboard_path)\n",
    "        \n",
    "        return dashboard_path\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error creating optimization dashboard: {e}\")\n",
    "        return None\n",
    "\n",
    "# Create optimization dashboard\n",
    "print(\"üìä Creating strategy optimization dashboard...\")\n",
    "dashboard_path = create_optimization_dashboard()\n",
    "\n",
    "if dashboard_path:\n",
    "    print(f\"‚úÖ Optimization dashboard created: {dashboard_path.name}\")\n",
    "    print(f\"üåê Open in browser: file://{dashboard_path.absolute()}\")\n",
    "\n",
    "# List all exported files\n",
    "print(\"\\nüìã Exported Files:\")\n",
    "chart_files = list(CHARTS_EXPORT_DIR.glob(\"*.html\"))\n",
    "optimization_files = [f for f in chart_files if 'optimization' in f.name.lower() or 'heatmap' in f.name.lower()]\n",
    "\n",
    "for chart_file in sorted(optimization_files):\n",
    "    size_kb = chart_file.stat().st_size / 1024\n",
    "    print(f\"  üìä {chart_file.name} ({size_kb:.1f}KB)\")\n",
    "\n",
    "print(f\"\\nüéØ Strategy optimization analysis completed!\")\n",
    "print(f\"üìÅ {len(optimization_files)} charts exported to: {CHARTS_EXPORT_DIR.absolute()}\")\n",
    "print(f\"\\nüìö Analysis Summary:\")\n",
    "print(f\"   ‚úÖ Parameter sensitivity analysis\")\n",
    "print(f\"   ‚úÖ Interactive heatmaps for parameter combinations\")\n",
    "print(f\"   ‚úÖ Strategy comparison and ranking\")\n",
    "print(f\"   ‚úÖ A/B testing framework for categorical parameters\")\n",
    "print(f\"   ‚úÖ Comprehensive optimization insights and recommendations\")\n",
    "print(f\"   ‚úÖ Interactive optimization dashboard\")\n",
    "\n",
    "print(f\"\\nüöÄ Next Steps:\")\n",
    "print(f\"   1. Review optimization dashboard for key insights\")\n",
    "print(f\"   2. Implement recommended parameter ranges\")\n",
    "print(f\"   3. Run risk_assessment.ipynb for detailed risk analysis\")\n",
    "print(f\"   4. Consider live testing with top-performing strategies\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéØ Strategy Optimization Analysis Summary\n",
    "\n",
    "This notebook provides comprehensive strategy optimization analysis with:\n",
    "\n",
    "‚úÖ **Parameter Sensitivity Analysis** - Correlation analysis and impact assessment  \n",
    "‚úÖ **Interactive Heatmaps** - Multi-dimensional parameter relationships visualization  \n",
    "‚úÖ **Strategy Comparison** - Multi-criteria ranking and performance analysis  \n",
    "‚úÖ **A/B Testing Framework** - Statistical significance testing for categorical parameters  \n",
    "‚úÖ **Optimization Insights** - Actionable parameter recommendations  \n",
    "‚úÖ **Comprehensive Dashboard** - Interactive overview of all optimization results  \n",
    "\n",
    "## üìä Key Features\n",
    "\n",
    "- **Robust Data Loading** - Intelligent source selection (CSV preferred, JSON fallback)\n",
    "- **Statistical Rigor** - Pearson/Spearman correlations, t-tests, Mann-Whitney U tests\n",
    "- **Visual Analytics** - Interactive heatmaps, scatter plots, and comparison charts\n",
    "- **Actionable Insights** - Clear recommendations based on statistical analysis\n",
    "- **Risk Assessment** - Drawdown analysis and risk-adjusted performance metrics\n",
    "\n",
    "## üìÅ Installation\n",
    "\n",
    "**Save as:** `notebooks/analysis/strategy_optimization.ipynb`\n",
    "\n",
    "**Dependencies:** `pip install plotly pandas numpy scipy scikit-learn`\n",
    "\n",
    "**Usage:** Run cells sequentially - notebook automatically detects optimization data\n",
    "\n",
    "## üîó Related Notebooks\n",
    "\n",
    "- **main_analysis.ipynb** - Core backtesting overview\n",
    "- **risk_assessment.ipynb** - Detailed risk metrics and drawdown analysis\n",
    "- **model_validation.ipynb** - Prediction accuracy and model performance\n",
    "\n",
    "---\n",
    "\n",
    "*Hockey Prediction System - Strategy Optimization Analysis*  \n",
    "*Location: notebooks/analysis/strategy_optimization.ipynb*  \n",
    "*Specialized analysis for parameter optimization and strategy comparison*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hockey-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
