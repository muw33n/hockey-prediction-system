{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üéØ Model Validation Analysis - NHL Elo Prediction System\n",
    "\n",
    "**Comprehensive model validation on complete 2024-25 NHL season data**\n",
    "\n",
    "**Scope:** 1312 games validation for prediction accuracy & calibration assessment\n",
    "\n",
    "**Business Goal:** Go/no-go deployment decision based on model performance\n",
    "\n",
    "---\n",
    "\n",
    "## üìã Analysis Overview\n",
    "\n",
    "1. **Setup & Configuration** - Libraries, paths, constants\n",
    "2. **Enhanced Data Discovery** - Robust CSV loading, fallbacks\n",
    "3. **Prediction Accuracy Analysis** - Overall accuracy, classification metrics\n",
    "4. **Calibration Analysis** - Probability reliability curves, Brier score\n",
    "5. **Classification Performance** - Confusion matrix, detailed breakdown\n",
    "6. **Temporal Analysis** - Performance stability over season\n",
    "7. **Market Comparison** - Model vs betting odds (where available)\n",
    "8. **Executive Summary** - Go/no-go deployment recommendations\n",
    "9. **Export & Visualization** - HTML charts, summary export\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. üöÄ Setup & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "from pathlib import Path\n",
    "\n",
    "# Use environment variables for paths\n",
    "if 'HOCKEY_LOGS_DIR' in os.environ:\n",
    "    logs_dir = Path(os.environ['HOCKEY_LOGS_DIR'])\n",
    "    project_root = Path(os.environ.get('HOCKEY_PROJECT_ROOT', '.'))\n",
    "else:\n",
    "    # Fallback for manual execution\n",
    "    project_root = Path('../../')\n",
    "    logs_dir = project_root / 'logs'\n",
    "\n",
    "# Ensure logs directory exists\n",
    "logs_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Configure logging\n",
    "log_file = logs_dir / 'model_validation_analysis.log'\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(log_file, encoding='utf-8'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Project paths\n",
    "logger.info(f\"?? Project root: {project_root.absolute()}\")\n",
    "logger.info(f\"?? Log file: {log_file.absolute()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('../../logs/model_validation_analysis.log', encoding='utf-8'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Project paths\n",
    "PROJECT_ROOT = Path.cwd().parent.parent if 'notebooks' in str(Path.cwd()) else Path.cwd()\n",
    "DATA_PATH = PROJECT_ROOT / 'models' / 'experiments'\n",
    "CHARTS_PATH = DATA_PATH / 'charts'\n",
    "LOGS_PATH = PROJECT_ROOT / 'logs'\n",
    "\n",
    "# Create directories if they don't exist\n",
    "CHARTS_PATH.mkdir(parents=True, exist_ok=True)\n",
    "LOGS_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"üìÅ Data path: {DATA_PATH}\")\n",
    "print(f\"üìä Charts export: {CHARTS_PATH}\")\n",
    "print(f\"üìù Logs path: {LOGS_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis configuration\n",
    "ANALYSIS_CONFIG = {\n",
    "    'target_accuracy': 0.55,  # Minimum acceptable accuracy\n",
    "    'training_benchmark': 0.588,  # Training accuracy benchmark\n",
    "    'confidence_thresholds': [0.5, 0.6, 0.7, 0.8, 0.9],\n",
    "    'max_brier_score': 0.25,  # Maximum acceptable Brier score\n",
    "    'chart_template': 'plotly_white',\n",
    "    'color_palette': ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd']\n",
    "}\n",
    "\n",
    "# Set Plotly template\n",
    "pio.templates.default = ANALYSIS_CONFIG['chart_template']\n",
    "\n",
    "logger.info(\"üöÄ Model Validation Analysis initialized\")\n",
    "logger.info(f\"üìÅ Data path: {DATA_PATH}\")\n",
    "logger.info(f\"üìä Charts export: {CHARTS_PATH}\")\n",
    "\n",
    "print(\"‚úÖ Configuration loaded successfully\")\n",
    "print(f\"üéØ Target accuracy: {ANALYSIS_CONFIG['target_accuracy']:.1%}\")\n",
    "print(f\"üìä Training benchmark: {ANALYSIS_CONFIG['training_benchmark']:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. üìÇ Enhanced Data Discovery & Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 2. ENHANCED DATA DISCOVERY & LOADING\n",
    "# =============================================================================\n",
    "\n",
    "def find_latest_validation_file(data_path: Path) -> Path:\n",
    "    \"\"\"Find the latest model validation data file\"\"\"\n",
    "    pattern = \"model_validation_complete_2025_*.csv\"\n",
    "    files = list(data_path.glob(pattern))\n",
    "    \n",
    "    if not files:\n",
    "        raise FileNotFoundError(f\"No validation data files found matching {pattern} in {data_path}\")\n",
    "    \n",
    "    # Sort by timestamp in filename\n",
    "    def extract_timestamp(filepath):\n",
    "        match = re.search(r'(\\d{8}_\\d{6})', str(filepath))\n",
    "        return match.group(1) if match else '00000000_000000'\n",
    "    \n",
    "    latest_file = max(files, key=extract_timestamp)\n",
    "    logger.info(f\"üìÇ Using validation data: {latest_file.name}\")\n",
    "    return latest_file\n",
    "\n",
    "def load_validation_data(file_path: Path) -> pd.DataFrame:\n",
    "    \"\"\"Load and validate model validation data with robust error handling\"\"\"\n",
    "    try:\n",
    "        # Try UTF-8 first, fallback to other encodings\n",
    "        encodings = ['utf-8', 'utf-8-sig', 'iso-8859-1', 'cp1252']\n",
    "        df = None\n",
    "        \n",
    "        for encoding in encodings:\n",
    "            try:\n",
    "                df = pd.read_csv(file_path, encoding=encoding)\n",
    "                logger.info(f\"‚úÖ Data loaded successfully with {encoding} encoding\")\n",
    "                break\n",
    "            except UnicodeDecodeError:\n",
    "                continue\n",
    "        \n",
    "        if df is None:\n",
    "            raise ValueError(\"Could not load data with any supported encoding\")\n",
    "        \n",
    "        # Data validation\n",
    "        logger.info(f\"üìä Dataset shape: {df.shape}\")\n",
    "        \n",
    "        # Required columns validation\n",
    "        required_cols = [\n",
    "            'game_id', 'date', 'home_team_name', 'away_team_name',\n",
    "            'actual_winner', 'predicted_winner', 'home_win_probability',\n",
    "            'away_win_probability', 'prediction_correct', 'model_confidence'\n",
    "        ]\n",
    "        \n",
    "        missing_cols = [col for col in required_cols if col not in df.columns]\n",
    "        if missing_cols:\n",
    "            raise ValueError(f\"Missing required columns: {missing_cols}\")\n",
    "        \n",
    "        # Convert data types\n",
    "        df['date'] = pd.to_datetime(df['date'])\n",
    "        df['prediction_correct'] = df['prediction_correct'].astype(bool)\n",
    "        df['has_odds_data'] = df.get('has_odds_data', False).astype(bool)\n",
    "        \n",
    "        # Data quality checks\n",
    "        logger.info(f\"üéØ Games with predictions: {len(df)}\")\n",
    "        logger.info(f\"üé≤ Games with odds data: {df['has_odds_data'].sum()}\")\n",
    "        logger.info(f\"üìÖ Date range: {df['date'].min()} to {df['date'].max()}\")\n",
    "        \n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"‚ùå Error loading validation data: {e}\")\n",
    "        raise\n",
    "\n",
    "print(\"‚úÖ Data loading functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "try:\n",
    "    validation_file = find_latest_validation_file(DATA_PATH)\n",
    "    df = load_validation_data(validation_file)\n",
    "    logger.info(\"‚úÖ Model validation data loaded successfully\")\n",
    "    \n",
    "    print(\"‚úÖ Data loaded successfully!\")\n",
    "    print(f\"üìä Dataset shape: {df.shape}\")\n",
    "    print(f\"üìÖ Date range: {df['date'].min().strftime('%Y-%m-%d')} to {df['date'].max().strftime('%Y-%m-%d')}\")\n",
    "    print(f\"üéØ Games with predictions: {len(df)}\")\n",
    "    print(f\"üé≤ Games with odds data: {df['has_odds_data'].sum()}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    logger.error(f\"‚ùå Failed to load validation data: {e}\")\n",
    "    print(f\"‚ùå Error: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data overview\n",
    "print(\"üìä Data Overview:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Columns: {list(df.columns)}\")\n",
    "print(\"\\nüîç Sample data:\")\n",
    "display(df.head())\n",
    "\n",
    "print(\"\\nüìà Basic statistics:\")\n",
    "print(f\"Overall accuracy: {df['prediction_correct'].mean():.1%}\")\n",
    "print(f\"Average model confidence: {df['model_confidence'].mean():.3f}\")\n",
    "print(f\"Home win rate (actual): {(df['actual_winner'] == df['home_team_name']).mean():.1%}\")\n",
    "print(f\"Home win rate (predicted): {(df['predicted_winner'] == df['home_team_name']).mean():.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. üéØ Prediction Accuracy Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 3. PREDICTION ACCURACY ANALYSIS\n",
    "# =============================================================================\n",
    "\n",
    "def analyze_prediction_accuracy(df: pd.DataFrame) -> dict:\n",
    "    \"\"\"Comprehensive prediction accuracy analysis\"\"\"\n",
    "    logger.info(\"üéØ Starting prediction accuracy analysis...\")\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # Overall accuracy\n",
    "    overall_accuracy = df['prediction_correct'].mean()\n",
    "    results['overall_accuracy'] = overall_accuracy\n",
    "    \n",
    "    # Home vs Away accuracy\n",
    "    home_predictions = df[df['predicted_winner'] == df['home_team_name']]\n",
    "    away_predictions = df[df['predicted_winner'] == df['away_team_name']]\n",
    "    \n",
    "    home_accuracy = home_predictions['prediction_correct'].mean() if len(home_predictions) > 0 else 0\n",
    "    away_accuracy = away_predictions['prediction_correct'].mean() if len(away_predictions) > 0 else 0\n",
    "    \n",
    "    results['home_accuracy'] = home_accuracy\n",
    "    results['away_accuracy'] = away_accuracy\n",
    "    results['home_predictions'] = len(home_predictions)\n",
    "    results['away_predictions'] = len(away_predictions)\n",
    "    \n",
    "    # Accuracy by confidence levels\n",
    "    confidence_analysis = {}\n",
    "    for threshold in ANALYSIS_CONFIG['confidence_thresholds']:\n",
    "        high_conf_games = df[df['model_confidence'] >= threshold]\n",
    "        if len(high_conf_games) > 0:\n",
    "            conf_accuracy = high_conf_games['prediction_correct'].mean()\n",
    "            confidence_analysis[threshold] = {\n",
    "                'accuracy': conf_accuracy,\n",
    "                'game_count': len(high_conf_games)\n",
    "            }\n",
    "    \n",
    "    results['confidence_analysis'] = confidence_analysis\n",
    "    \n",
    "    logger.info(f\"üìä Overall accuracy: {overall_accuracy:.3f}\")\n",
    "    logger.info(f\"üè† Home predictions accuracy: {home_accuracy:.3f} ({len(home_predictions)} games)\")\n",
    "    logger.info(f\"‚úàÔ∏è Away predictions accuracy: {away_accuracy:.3f} ({len(away_predictions)} games)\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Perform accuracy analysis\n",
    "accuracy_results = analyze_prediction_accuracy(df)\n",
    "\n",
    "print(\"‚úÖ Accuracy analysis completed\")\n",
    "print(f\"üìä Overall accuracy: {accuracy_results['overall_accuracy']:.1%}\")\n",
    "print(f\"üè† Home predictions: {accuracy_results['home_accuracy']:.1%} ({accuracy_results['home_predictions']} games)\")\n",
    "print(f\"‚úàÔ∏è Away predictions: {accuracy_results['away_accuracy']:.1%} ({accuracy_results['away_predictions']} games)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create accuracy visualizations\n",
    "def create_accuracy_charts(df: pd.DataFrame, results: dict):\n",
    "    \"\"\"Create comprehensive accuracy visualizations\"\"\"\n",
    "    \n",
    "    # 1. Overall accuracy vs benchmarks\n",
    "    fig_overview = go.Figure()\n",
    "    \n",
    "    accuracies = [\n",
    "        results['overall_accuracy'],\n",
    "        ANALYSIS_CONFIG['target_accuracy'],\n",
    "        ANALYSIS_CONFIG['training_benchmark']\n",
    "    ]\n",
    "    labels = ['Model Performance', 'Target Threshold', 'Training Benchmark']\n",
    "    colors = ['#2E86AB', '#A23B72', '#F18F01']\n",
    "    \n",
    "    fig_overview.add_trace(go.Bar(\n",
    "        x=labels,\n",
    "        y=accuracies,\n",
    "        marker_color=colors,\n",
    "        text=[f\"{acc:.1%}\" for acc in accuracies],\n",
    "        textposition='auto'\n",
    "    ))\n",
    "    \n",
    "    fig_overview.update_layout(\n",
    "        title=\"üéØ Model Accuracy vs Benchmarks\",\n",
    "        yaxis_title=\"Accuracy\",\n",
    "        yaxis=dict(range=[0, 1], tickformat='.0%'),\n",
    "        height=400\n",
    "    )\n",
    "    \n",
    "    # 2. Home vs Away accuracy\n",
    "    fig_home_away = go.Figure()\n",
    "    \n",
    "    fig_home_away.add_trace(go.Bar(\n",
    "        name='Home Predictions',\n",
    "        x=['Accuracy'],\n",
    "        y=[results['home_accuracy']],\n",
    "        marker_color='#1f77b4',\n",
    "        text=f\"{results['home_accuracy']:.1%}\",\n",
    "        textposition='auto'\n",
    "    ))\n",
    "    \n",
    "    fig_home_away.add_trace(go.Bar(\n",
    "        name='Away Predictions', \n",
    "        x=['Accuracy'],\n",
    "        y=[results['away_accuracy']],\n",
    "        marker_color='#ff7f0e',\n",
    "        text=f\"{results['away_accuracy']:.1%}\",\n",
    "        textposition='auto'\n",
    "    ))\n",
    "    \n",
    "    fig_home_away.update_layout(\n",
    "        title=\"üè† Home vs Away Prediction Accuracy\",\n",
    "        yaxis_title=\"Accuracy\",\n",
    "        yaxis=dict(range=[0, 1], tickformat='.0%'),\n",
    "        height=400\n",
    "    )\n",
    "    \n",
    "    # 3. Confidence-based accuracy\n",
    "    conf_thresholds = list(results['confidence_analysis'].keys())\n",
    "    conf_accuracies = [results['confidence_analysis'][t]['accuracy'] for t in conf_thresholds]\n",
    "    conf_counts = [results['confidence_analysis'][t]['game_count'] for t in conf_thresholds]\n",
    "    \n",
    "    fig_conf = make_subplots(specs=[[{\"secondary_y\": True}]])\n",
    "    \n",
    "    fig_conf.add_trace(\n",
    "        go.Scatter(\n",
    "            x=conf_thresholds,\n",
    "            y=conf_accuracies,\n",
    "            mode='lines+markers',\n",
    "            name='Accuracy',\n",
    "            line=dict(color='#2E86AB', width=3),\n",
    "            marker=dict(size=8)\n",
    "        ),\n",
    "        secondary_y=False\n",
    "    )\n",
    "    \n",
    "    fig_conf.add_trace(\n",
    "        go.Bar(\n",
    "            x=conf_thresholds,\n",
    "            y=conf_counts,\n",
    "            name='Game Count',\n",
    "            opacity=0.3,\n",
    "            marker_color='#F18F01'\n",
    "        ),\n",
    "        secondary_y=True\n",
    "    )\n",
    "    \n",
    "    fig_conf.update_layout(title=\"üìà Accuracy by Model Confidence\")\n",
    "    fig_conf.update_xaxes(title_text=\"Confidence Threshold\")\n",
    "    fig_conf.update_yaxes(title_text=\"Accuracy\", secondary_y=False, tickformat='.0%')\n",
    "    fig_conf.update_yaxes(title_text=\"Number of Games\", secondary_y=True)\n",
    "    \n",
    "    return fig_overview, fig_home_away, fig_conf\n",
    "\n",
    "accuracy_charts = create_accuracy_charts(df, accuracy_results)\n",
    "\n",
    "print(\"üìä Displaying accuracy charts...\")\n",
    "for i, chart in enumerate(accuracy_charts, 1):\n",
    "    chart.show()\n",
    "    print(f\"Chart {i} displayed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. üé≤ Calibration Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 4. CALIBRATION ANALYSIS\n",
    "# =============================================================================\n",
    "\n",
    "def analyze_model_calibration(df: pd.DataFrame) -> dict:\n",
    "    \"\"\"Analyze probability calibration quality\"\"\"\n",
    "    logger.info(\"üé≤ Starting calibration analysis...\")\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # Prepare data for calibration\n",
    "    y_true = (df['actual_winner'] == df['home_team_name']).astype(int)\n",
    "    y_prob = df['home_win_probability'].values\n",
    "    \n",
    "    # Brier score\n",
    "    brier_score = brier_score_loss(y_true, y_prob)\n",
    "    results['brier_score'] = brier_score\n",
    "    \n",
    "    # Calibration curve\n",
    "    fraction_of_positives, mean_predicted_value = calibration_curve(\n",
    "        y_true, y_prob, n_bins=10, strategy='quantile'\n",
    "    )\n",
    "    \n",
    "    results['calibration_curve'] = {\n",
    "        'fraction_of_positives': fraction_of_positives,\n",
    "        'mean_predicted_value': mean_predicted_value\n",
    "    }\n",
    "    \n",
    "    # Reliability metrics\n",
    "    reliability = np.mean((fraction_of_positives - mean_predicted_value) ** 2)\n",
    "    resolution = np.mean((fraction_of_positives - np.mean(y_true)) ** 2)\n",
    "    uncertainty = np.mean(y_true) * (1 - np.mean(y_true))\n",
    "    \n",
    "    results['reliability'] = reliability\n",
    "    results['resolution'] = resolution\n",
    "    results['uncertainty'] = uncertainty\n",
    "    \n",
    "    # Calibration by probability bins\n",
    "    bins = np.linspace(0, 1, 11)\n",
    "    bin_centers = (bins[:-1] + bins[1:]) / 2\n",
    "    bin_counts = []\n",
    "    bin_accuracies = []\n",
    "    \n",
    "    for i in range(len(bins)-1):\n",
    "        mask = (y_prob >= bins[i]) & (y_prob < bins[i+1])\n",
    "        if i == len(bins)-2:  # Include 1.0 in the last bin\n",
    "            mask = (y_prob >= bins[i]) & (y_prob <= bins[i+1])\n",
    "        \n",
    "        if np.sum(mask) > 0:\n",
    "            bin_counts.append(np.sum(mask))\n",
    "            bin_accuracies.append(np.mean(y_true[mask]))\n",
    "        else:\n",
    "            bin_counts.append(0)\n",
    "            bin_accuracies.append(0)\n",
    "    \n",
    "    results['probability_bins'] = {\n",
    "        'bin_centers': bin_centers,\n",
    "        'bin_counts': bin_counts,\n",
    "        'bin_accuracies': bin_accuracies\n",
    "    }\n",
    "    \n",
    "    logger.info(f\"üé≤ Brier Score: {brier_score:.4f}\")\n",
    "    logger.info(f\"üìä Reliability: {reliability:.4f}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "calibration_results = analyze_model_calibration(df)\n",
    "\n",
    "print(\"‚úÖ Calibration analysis completed\")\n",
    "print(f\"üé≤ Brier Score: {calibration_results['brier_score']:.4f}\")\n",
    "print(f\"üìä Reliability: {calibration_results['reliability']:.4f}\")\n",
    "print(f\"üéØ Target Brier Score: ‚â§ {ANALYSIS_CONFIG['max_brier_score']:.3f}\")\n",
    "print(f\"‚úÖ Meets target: {'YES' if calibration_results['brier_score'] <= ANALYSIS_CONFIG['max_brier_score'] else 'NO'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create calibration visualizations\n",
    "def create_calibration_charts(results: dict):\n",
    "    \"\"\"Create calibration analysis charts\"\"\"\n",
    "    \n",
    "    # 1. Calibration curve\n",
    "    fig_cal = go.Figure()\n",
    "    \n",
    "    # Perfect calibration line\n",
    "    fig_cal.add_trace(go.Scatter(\n",
    "        x=[0, 1],\n",
    "        y=[0, 1],\n",
    "        mode='lines',\n",
    "        name='Perfect Calibration',\n",
    "        line=dict(color='gray', dash='dash', width=2)\n",
    "    ))\n",
    "    \n",
    "    # Actual calibration\n",
    "    cal_data = results['calibration_curve']\n",
    "    fig_cal.add_trace(go.Scatter(\n",
    "        x=cal_data['mean_predicted_value'],\n",
    "        y=cal_data['fraction_of_positives'],\n",
    "        mode='lines+markers',\n",
    "        name='Model Calibration',\n",
    "        line=dict(color='#2E86AB', width=3),\n",
    "        marker=dict(size=8)\n",
    "    ))\n",
    "    \n",
    "    fig_cal.update_layout(\n",
    "        title=\"üéØ Probability Calibration Curve\",\n",
    "        xaxis_title=\"Mean Predicted Probability\",\n",
    "        yaxis_title=\"Fraction of Positives\",\n",
    "        xaxis=dict(range=[0, 1]),\n",
    "        yaxis=dict(range=[0, 1])\n",
    "    )\n",
    "    \n",
    "    # 2. Reliability diagram\n",
    "    bin_data = results['probability_bins']\n",
    "    \n",
    "    fig_rel = make_subplots(specs=[[{\"secondary_y\": True}]])\n",
    "    \n",
    "    fig_rel.add_trace(\n",
    "        go.Bar(\n",
    "            x=bin_data['bin_centers'],\n",
    "            y=bin_data['bin_counts'],\n",
    "            name='Frequency',\n",
    "            opacity=0.3,\n",
    "            marker_color='#F18F01',\n",
    "            width=0.08\n",
    "        ),\n",
    "        secondary_y=True\n",
    "    )\n",
    "    \n",
    "    fig_rel.add_trace(\n",
    "        go.Scatter(\n",
    "            x=bin_data['bin_centers'],\n",
    "            y=bin_data['bin_accuracies'],\n",
    "            mode='lines+markers',\n",
    "            name='Observed Frequency',\n",
    "            line=dict(color='#2E86AB', width=3),\n",
    "            marker=dict(size=10)\n",
    "        ),\n",
    "        secondary_y=False\n",
    "    )\n",
    "    \n",
    "    # Perfect calibration line\n",
    "    fig_rel.add_trace(\n",
    "        go.Scatter(\n",
    "            x=[0, 1],\n",
    "            y=[0, 1],\n",
    "            mode='lines',\n",
    "            name='Perfect Calibration',\n",
    "            line=dict(color='gray', dash='dash', width=2)\n",
    "        ),\n",
    "        secondary_y=False\n",
    "    )\n",
    "    \n",
    "    fig_rel.update_layout(title=\"üìä Reliability Diagram\")\n",
    "    fig_rel.update_xaxes(title_text=\"Predicted Probability\")\n",
    "    fig_rel.update_yaxes(title_text=\"Observed Frequency\", secondary_y=False)\n",
    "    fig_rel.update_yaxes(title_text=\"Count\", secondary_y=True)\n",
    "    \n",
    "    return fig_cal, fig_rel\n",
    "\n",
    "calibration_charts = create_calibration_charts(calibration_results)\n",
    "\n",
    "print(\"üìä Displaying calibration charts...\")\n",
    "for i, chart in enumerate(calibration_charts, 1):\n",
    "    chart.show()\n",
    "    print(f\"Calibration chart {i} displayed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. üîç Classification Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 5. CONFUSION MATRIX & CLASSIFICATION PERFORMANCE\n",
    "# =============================================================================\n",
    "\n",
    "def analyze_classification_performance(df: pd.DataFrame) -> dict:\n",
    "    \"\"\"Detailed classification performance analysis\"\"\"\n",
    "    logger.info(\"üîç Starting classification performance analysis...\")\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # Prepare labels\n",
    "    y_true = df['actual_winner'].values\n",
    "    y_pred = df['predicted_winner'].values\n",
    "    \n",
    "    # Get unique teams for confusion matrix\n",
    "    unique_winners = sorted(list(set(list(y_true) + list(y_pred))))\n",
    "    \n",
    "    # Confusion matrix - only use teams that actually appear as winners\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=unique_winners)\n",
    "    results['confusion_matrix'] = cm\n",
    "    results['team_labels'] = unique_winners\n",
    "    \n",
    "    # Classification report\n",
    "    clf_report = classification_report(y_true, y_pred, output_dict=True, zero_division=0)\n",
    "    results['classification_report'] = clf_report\n",
    "    \n",
    "    # Team-specific performance\n",
    "    team_performance = {}\n",
    "    for team in unique_winners:\n",
    "        team_games = df[(df['home_team_name'] == team) | (df['away_team_name'] == team)]\n",
    "        if len(team_games) > 0:\n",
    "            team_correct = team_games['prediction_correct'].sum()\n",
    "            team_total = len(team_games)\n",
    "            team_performance[team] = {\n",
    "                'accuracy': team_correct / team_total,\n",
    "                'games': team_total,\n",
    "                'correct': team_correct\n",
    "            }\n",
    "    \n",
    "    results['team_performance'] = team_performance\n",
    "    \n",
    "    logger.info(f\"üìä Macro avg F1-score: {clf_report['macro avg']['f1-score']:.3f}\")\n",
    "    logger.info(f\"üìä Weighted avg F1-score: {clf_report['weighted avg']['f1-score']:.3f}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "classification_results = analyze_classification_performance(df)\n",
    "\n",
    "print(\"‚úÖ Classification analysis completed\")\n",
    "print(f\"üìä Total teams analyzed: {len(classification_results['team_labels'])}\")\n",
    "print(f\"üìä Macro avg F1-score: {classification_results['classification_report']['macro avg']['f1-score']:.3f}\")\n",
    "print(f\"üìä Weighted avg F1-score: {classification_results['classification_report']['weighted avg']['f1-score']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. üìÖ Temporal Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 6. TEMPORAL PERFORMANCE ANALYSIS\n",
    "# =============================================================================\n",
    "\n",
    "def analyze_temporal_performance(df: pd.DataFrame) -> dict:\n",
    "    \"\"\"Analyze model performance over time\"\"\"\n",
    "    logger.info(\"üìÖ Starting temporal performance analysis...\")\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # Sort by date\n",
    "    df_sorted = df.sort_values('date')\n",
    "    \n",
    "    # Monthly performance\n",
    "    df_sorted['month'] = df_sorted['date'].dt.to_period('M')\n",
    "    monthly_performance = df_sorted.groupby('month').agg({\n",
    "        'prediction_correct': ['mean', 'count'],\n",
    "        'model_confidence': 'mean'\n",
    "    }).round(3)\n",
    "    \n",
    "    monthly_performance.columns = ['accuracy', 'games', 'avg_confidence']\n",
    "    monthly_performance = monthly_performance.reset_index()\n",
    "    monthly_performance['month'] = monthly_performance['month'].astype(str)\n",
    "    \n",
    "    results['monthly_performance'] = monthly_performance\n",
    "    \n",
    "    # Rolling 30-game accuracy\n",
    "    window_size = 30\n",
    "    rolling_accuracy = df_sorted['prediction_correct'].rolling(window=window_size, min_periods=10).mean()\n",
    "    \n",
    "    results['rolling_accuracy'] = {\n",
    "        'values': rolling_accuracy.values,\n",
    "        'dates': df_sorted['date'].values,\n",
    "        'window_size': window_size\n",
    "    }\n",
    "    \n",
    "    # Early vs Late season comparison  \n",
    "    total_games = len(df_sorted)\n",
    "    early_season = df_sorted.iloc[:total_games//2]\n",
    "    late_season = df_sorted.iloc[total_games//2:]\n",
    "    \n",
    "    results['seasonal_comparison'] = {\n",
    "        'early_accuracy': early_season['prediction_correct'].mean(),\n",
    "        'late_accuracy': late_season['prediction_correct'].mean(),\n",
    "        'early_games': len(early_season),\n",
    "        'late_games': len(late_season)\n",
    "    }\n",
    "    \n",
    "    logger.info(f\"üìä Early season accuracy: {results['seasonal_comparison']['early_accuracy']:.3f}\")\n",
    "    logger.info(f\"üìä Late season accuracy: {results['seasonal_comparison']['late_accuracy']:.3f}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "temporal_results = analyze_temporal_performance(df)\n",
    "\n",
    "print(\"‚úÖ Temporal analysis completed\")\n",
    "seasonal = temporal_results['seasonal_comparison']\n",
    "print(f\"üìä Early season ({seasonal['early_games']} games): {seasonal['early_accuracy']:.1%}\")\n",
    "print(f\"üìä Late season ({seasonal['late_games']} games): {seasonal['late_accuracy']:.1%}\")\n",
    "print(f\"üìä Monthly trends: {len(temporal_results['monthly_performance'])} months analyzed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create temporal visualizations\n",
    "def create_temporal_charts(results: dict):\n",
    "    \"\"\"Create temporal analysis charts\"\"\"\n",
    "    \n",
    "    # 1. Monthly performance\n",
    "    monthly = results['monthly_performance']\n",
    "    \n",
    "    fig_monthly = make_subplots(specs=[[{\"secondary_y\": True}]])\n",
    "    \n",
    "    fig_monthly.add_trace(\n",
    "        go.Scatter(\n",
    "            x=monthly['month'],\n",
    "            y=monthly['accuracy'],\n",
    "            mode='lines+markers',\n",
    "            name='Monthly Accuracy',\n",
    "            line=dict(color='#2E86AB', width=3),\n",
    "            marker=dict(size=8)\n",
    "        ),\n",
    "        secondary_y=False\n",
    "    )\n",
    "    \n",
    "    fig_monthly.add_trace(\n",
    "        go.Bar(\n",
    "            x=monthly['month'],\n",
    "            y=monthly['games'],\n",
    "            name='Games per Month',\n",
    "            opacity=0.3,\n",
    "            marker_color='#F18F01'\n",
    "        ),\n",
    "        secondary_y=True\n",
    "    )\n",
    "    \n",
    "    fig_monthly.update_layout(title=\"üìÖ Monthly Performance Trends\")\n",
    "    fig_monthly.update_xaxes(title_text=\"Month\")\n",
    "    fig_monthly.update_yaxes(title_text=\"Accuracy\", secondary_y=False, tickformat='.0%')\n",
    "    fig_monthly.update_yaxes(title_text=\"Games Count\", secondary_y=True)\n",
    "    \n",
    "    # 2. Rolling accuracy\n",
    "    rolling = results['rolling_accuracy']\n",
    "    \n",
    "    fig_rolling = go.Figure()\n",
    "    \n",
    "    fig_rolling.add_trace(go.Scatter(\n",
    "        x=rolling['dates'],\n",
    "        y=rolling['values'],\n",
    "        mode='lines',\n",
    "        name=f'{rolling[\"window_size\"]}-Game Rolling Accuracy',\n",
    "        line=dict(color='#2E86AB', width=2)\n",
    "    ))\n",
    "    \n",
    "    # Add target line\n",
    "    fig_rolling.add_trace(go.Scatter(\n",
    "        x=[rolling['dates'][0], rolling['dates'][-1]],\n",
    "        y=[ANALYSIS_CONFIG['target_accuracy'], ANALYSIS_CONFIG['target_accuracy']],\n",
    "        mode='lines',\n",
    "        name='Target Accuracy',\n",
    "        line=dict(color='red', dash='dash')\n",
    "    ))\n",
    "    \n",
    "    fig_rolling.update_layout(\n",
    "        title=f\"üîÑ Rolling {rolling['window_size']}-Game Accuracy\",\n",
    "        xaxis_title=\"Date\",\n",
    "        yaxis_title=\"Accuracy\",\n",
    "        yaxis=dict(tickformat='.0%')\n",
    "    )\n",
    "    \n",
    "    return fig_monthly, fig_rolling\n",
    "\n",
    "temporal_charts = create_temporal_charts(temporal_results)\n",
    "\n",
    "print(\"üìä Displaying temporal charts...\")\n",
    "for i, chart in enumerate(temporal_charts, 1):\n",
    "    chart.show()\n",
    "    print(f\"Temporal chart {i} displayed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. üí∞ Market Comparison Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 7. MARKET COMPARISON ANALYSIS\n",
    "# =============================================================================\n",
    "\n",
    "def analyze_market_comparison(df: pd.DataFrame) -> dict:\n",
    "    \"\"\"Compare model performance vs betting market\"\"\"\n",
    "    logger.info(\"üí∞ Starting market comparison analysis...\")\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # Filter games with odds data\n",
    "    odds_df = df[df['has_odds_data'] == True].copy()\n",
    "    \n",
    "    if len(odds_df) == 0:\n",
    "        logger.warning(\"‚ö†Ô∏è No odds data available for market comparison\")\n",
    "        results['has_data'] = False\n",
    "        return results\n",
    "    \n",
    "    results['has_data'] = True\n",
    "    results['odds_coverage'] = len(odds_df) / len(df)\n",
    "    \n",
    "    # Market accuracy (using implied probabilities)\n",
    "    market_predictions = []\n",
    "    for _, row in odds_df.iterrows():\n",
    "        if pd.notna(row.get('home_implied_prob')) and pd.notna(row.get('away_implied_prob')):\n",
    "            if row['home_implied_prob'] > row['away_implied_prob']:\n",
    "                market_predictions.append(row['home_team_name'])\n",
    "            else:\n",
    "                market_predictions.append(row['away_team_name'])\n",
    "        else:\n",
    "            market_predictions.append(None)  # No prediction if missing odds\n",
    "    \n",
    "    odds_df['market_prediction'] = market_predictions\n",
    "    \n",
    "    # Only analyze games with valid market predictions\n",
    "    valid_market_df = odds_df.dropna(subset=['market_prediction'])\n",
    "    \n",
    "    if len(valid_market_df) == 0:\n",
    "        logger.warning(\"‚ö†Ô∏è No valid market predictions available\")\n",
    "        results['has_data'] = False\n",
    "        return results\n",
    "    \n",
    "    valid_market_df['market_correct'] = valid_market_df['market_prediction'] == valid_market_df['actual_winner']\n",
    "    \n",
    "    market_accuracy = valid_market_df['market_correct'].mean()\n",
    "    model_accuracy_on_odds = valid_market_df['prediction_correct'].mean()\n",
    "    \n",
    "    results['market_accuracy'] = market_accuracy\n",
    "    results['model_accuracy_on_odds'] = model_accuracy_on_odds\n",
    "    results['accuracy_difference'] = model_accuracy_on_odds - market_accuracy\n",
    "    results['valid_comparisons'] = len(valid_market_df)\n",
    "    \n",
    "    # Calibration comparison (if we have the necessary columns)\n",
    "    if 'home_implied_prob' in valid_market_df.columns and 'away_implied_prob' in valid_market_df.columns:\n",
    "        market_probs = valid_market_df['home_implied_prob'].values\n",
    "        model_probs = valid_market_df['home_win_probability'].values\n",
    "        y_true = (valid_market_df['actual_winner'] == valid_market_df['home_team_name']).astype(int)\n",
    "        \n",
    "        # Only calculate if we have valid probabilities\n",
    "        valid_probs_mask = pd.notna(market_probs) & pd.notna(model_probs)\n",
    "        if np.sum(valid_probs_mask) > 0:\n",
    "            market_brier = brier_score_loss(y_true[valid_probs_mask], market_probs[valid_probs_mask])\n",
    "            model_brier = brier_score_loss(y_true[valid_probs_mask], model_probs[valid_probs_mask])\n",
    "            \n",
    "            results['market_brier'] = market_brier\n",
    "            results['model_brier'] = model_brier\n",
    "            results['brier_improvement'] = market_brier - model_brier\n",
    "    \n",
    "    logger.info(f\"üí∞ Market accuracy: {market_accuracy:.3f}\")\n",
    "    logger.info(f\"ü§ñ Model accuracy (on odds games): {model_accuracy_on_odds:.3f}\")\n",
    "    logger.info(f\"üìä Accuracy difference: {results['accuracy_difference']:+.3f}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "market_results = analyze_market_comparison(df)\n",
    "\n",
    "if market_results.get('has_data'):\n",
    "    print(\"‚úÖ Market comparison completed\")\n",
    "    print(f\"üí∞ Market accuracy: {market_results['market_accuracy']:.1%}\")\n",
    "    print(f\"ü§ñ Model accuracy (on odds games): {market_results['model_accuracy_on_odds']:.1%}\")\n",
    "    print(f\"üìä Accuracy difference: {market_results['accuracy_difference']:+.1%}\")\n",
    "    print(f\"üìà Valid comparisons: {market_results['valid_comparisons']} games\")\n",
    "    \n",
    "    if 'brier_improvement' in market_results:\n",
    "        print(f\"üé≤ Brier score improvement: {market_results['brier_improvement']:+.4f}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No market comparison data available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. üìã Executive Summary & Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 8. EXECUTIVE SUMMARY & RECOMMENDATIONS\n",
    "# =============================================================================\n",
    "\n",
    "def generate_executive_summary(\n",
    "    accuracy_results: dict,\n",
    "    calibration_results: dict,\n",
    "    temporal_results: dict,\n",
    "    market_results: dict\n",
    ") -> dict:\n",
    "    \"\"\"Generate executive summary with go/no-go recommendations\"\"\"\n",
    "    logger.info(\"üìã Generating executive summary...\")\n",
    "    \n",
    "    summary = {\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "        'dataset_info': {\n",
    "            'total_games': len(df),\n",
    "            'date_range': f\"{df['date'].min().strftime('%Y-%m-%d')} to {df['date'].max().strftime('%Y-%m-%d')}\",\n",
    "            'odds_coverage': f\"{market_results.get('odds_coverage', 0):.1%}\" if market_results.get('has_data') else \"0%\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Performance Assessment\n",
    "    overall_accuracy = accuracy_results['overall_accuracy']\n",
    "    target_accuracy = ANALYSIS_CONFIG['target_accuracy']\n",
    "    training_benchmark = ANALYSIS_CONFIG['training_benchmark']\n",
    "    \n",
    "    performance_grade = 'A' if overall_accuracy >= training_benchmark else \\\n",
    "                       'B' if overall_accuracy >= target_accuracy else \\\n",
    "                       'C' if overall_accuracy >= 0.52 else 'D'\n",
    "    \n",
    "    summary['performance_assessment'] = {\n",
    "        'overall_accuracy': overall_accuracy,\n",
    "        'vs_target': overall_accuracy - target_accuracy,\n",
    "        'vs_training': overall_accuracy - training_benchmark,\n",
    "        'grade': performance_grade,\n",
    "        'meets_target': overall_accuracy >= target_accuracy\n",
    "    }\n",
    "    \n",
    "    # Calibration Assessment\n",
    "    brier_score = calibration_results['brier_score']\n",
    "    max_brier = ANALYSIS_CONFIG['max_brier_score']\n",
    "    \n",
    "    calibration_grade = 'A' if brier_score <= 0.20 else \\\n",
    "                       'B' if brier_score <= max_brier else \\\n",
    "                       'C' if brier_score <= 0.30 else 'D'\n",
    "    \n",
    "    summary['calibration_assessment'] = {\n",
    "        'brier_score': brier_score,\n",
    "        'target_brier': max_brier,\n",
    "        'grade': calibration_grade,\n",
    "        'well_calibrated': brier_score <= max_brier\n",
    "    }\n",
    "    \n",
    "    # Stability Assessment\n",
    "    early_acc = temporal_results['seasonal_comparison']['early_accuracy']\n",
    "    late_acc = temporal_results['seasonal_comparison']['late_accuracy']\n",
    "    stability_score = 1 - abs(early_acc - late_acc)\n",
    "    \n",
    "    summary['stability_assessment'] = {\n",
    "        'early_season_accuracy': early_acc,\n",
    "        'late_season_accuracy': late_acc,\n",
    "        'stability_score': stability_score,\n",
    "        'is_stable': stability_score >= 0.85\n",
    "    }\n",
    "    \n",
    "    # Market Competitiveness\n",
    "    if market_results.get('has_data'):\n",
    "        market_edge = market_results['accuracy_difference']\n",
    "        summary['market_competitiveness'] = {\n",
    "            'model_vs_market': market_edge,\n",
    "            'has_edge': market_edge > 0,\n",
    "            'brier_improvement': market_results.get('brier_improvement', 0)\n",
    "        }\n",
    "    else:\n",
    "        summary['market_competitiveness'] = {'has_data': False}\n",
    "    \n",
    "    # Overall Recommendation\n",
    "    criteria_met = 0\n",
    "    total_criteria = 4\n",
    "    \n",
    "    if summary['performance_assessment']['meets_target']:\n",
    "        criteria_met += 1\n",
    "    if summary['calibration_assessment']['well_calibrated']:\n",
    "        criteria_met += 1\n",
    "    if summary['stability_assessment']['is_stable']:\n",
    "        criteria_met += 1\n",
    "    if market_results.get('has_data') and summary['market_competitiveness']['has_edge']:\n",
    "        criteria_met += 1\n",
    "    elif not market_results.get('has_data'):\n",
    "        total_criteria = 3  # Adjust if no market data\n",
    "    \n",
    "    recommendation_score = criteria_met / total_criteria\n",
    "    \n",
    "    if recommendation_score >= 0.75:\n",
    "        recommendation = \"GO - Deploy to Production\"\n",
    "        recommendation_color = \"green\"\n",
    "    elif recommendation_score >= 0.5:\n",
    "        recommendation = \"CONDITIONAL GO - Deploy with Monitoring\"\n",
    "        recommendation_color = \"orange\"\n",
    "    else:\n",
    "        recommendation = \"NO GO - Needs Improvement\"\n",
    "        recommendation_color = \"red\"\n",
    "    \n",
    "    summary['final_recommendation'] = {\n",
    "        'recommendation': recommendation,\n",
    "        'score': recommendation_score,\n",
    "        'color': recommendation_color,\n",
    "        'criteria_met': f\"{criteria_met}/{total_criteria}\"\n",
    "    }\n",
    "    \n",
    "    return summary\n",
    "\n",
    "executive_summary = generate_executive_summary(\n",
    "    accuracy_results, calibration_results, temporal_results, market_results\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Executive summary generated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. üíæ Export Visualizations & Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 9. EXPORT VISUALIZATIONS & SUMMARY\n",
    "# =============================================================================\n",
    "\n",
    "def export_all_charts():\n",
    "    \"\"\"Export all visualizations to HTML files\"\"\"\n",
    "    logger.info(\"üíæ Exporting visualizations...\")\n",
    "    \n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    # Export accuracy charts\n",
    "    for i, chart in enumerate(accuracy_charts):\n",
    "        filename = f\"model_validation_accuracy_{i+1}_{timestamp}.html\"\n",
    "        chart.write_html(CHARTS_PATH / filename)\n",
    "    \n",
    "    # Export calibration charts\n",
    "    for i, chart in enumerate(calibration_charts):\n",
    "        filename = f\"model_validation_calibration_{i+1}_{timestamp}.html\"\n",
    "        chart.write_html(CHARTS_PATH / filename)\n",
    "    \n",
    "    # Export temporal charts\n",
    "    for i, chart in enumerate(temporal_charts):\n",
    "        filename = f\"model_validation_temporal_{i+1}_{timestamp}.html\"\n",
    "        chart.write_html(CHARTS_PATH / filename)\n",
    "    \n",
    "    logger.info(f\"‚úÖ Charts exported to {CHARTS_PATH}\")\n",
    "    return timestamp\n",
    "\n",
    "def export_executive_summary(summary: dict, timestamp: str):\n",
    "    \"\"\"Export executive summary to JSON\"\"\"\n",
    "    filename = f\"model_validation_summary_{timestamp}.json\"\n",
    "    \n",
    "    with open(CHARTS_PATH / filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(summary, f, indent=2, ensure_ascii=False, default=str)\n",
    "    \n",
    "    logger.info(f\"üìã Executive summary exported: {filename}\")\n",
    "    return filename\n",
    "\n",
    "# Export everything\n",
    "export_timestamp = export_all_charts()\n",
    "summary_filename = export_executive_summary(executive_summary, export_timestamp)\n",
    "\n",
    "print(f\"‚úÖ All exports completed\")\n",
    "print(f\"üìÅ Charts exported to: {CHARTS_PATH}\")\n",
    "print(f\"üìã Summary saved as: {summary_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. üéØ Final Results Display\n",
    "\n",
    "### Executive Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 10. FINAL RESULTS DISPLAY\n",
    "# =============================================================================\n",
    "\n",
    "print(\"üéØ MODEL VALIDATION ANALYSIS - EXECUTIVE SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"üìä Dataset: {executive_summary['dataset_info']['total_games']} games\")\n",
    "print(f\"üìÖ Period: {executive_summary['dataset_info']['date_range']}\")\n",
    "print(f\"üí∞ Odds Coverage: {executive_summary['dataset_info']['odds_coverage']}\")\n",
    "print()\n",
    "\n",
    "print(\"üéØ PERFORMANCE ASSESSMENT\")\n",
    "print(\"-\" * 30)\n",
    "perf = executive_summary['performance_assessment']\n",
    "print(f\"Overall Accuracy: {perf['overall_accuracy']:.1%} (Grade: {perf['grade']})\")\n",
    "print(f\"vs Target ({ANALYSIS_CONFIG['target_accuracy']:.1%}): {perf['vs_target']:+.1%}\")\n",
    "print(f\"vs Training ({ANALYSIS_CONFIG['training_benchmark']:.1%}): {perf['vs_training']:+.1%}\")\n",
    "print(f\"Meets Target: {'‚úÖ YES' if perf['meets_target'] else '‚ùå NO'}\")\n",
    "print()\n",
    "\n",
    "print(\"üé≤ CALIBRATION ASSESSMENT\")\n",
    "print(\"-\" * 30)\n",
    "cal = executive_summary['calibration_assessment']\n",
    "print(f\"Brier Score: {cal['brier_score']:.4f} (Grade: {cal['grade']})\")\n",
    "print(f\"Target: ‚â§ {cal['target_brier']:.3f}\")\n",
    "print(f\"Well Calibrated: {'‚úÖ YES' if cal['well_calibrated'] else '‚ùå NO'}\")\n",
    "print()\n",
    "\n",
    "print(\"üìÖ STABILITY ASSESSMENT\")\n",
    "print(\"-\" * 30)\n",
    "stab = executive_summary['stability_assessment']\n",
    "print(f\"Early Season: {stab['early_season_accuracy']:.1%}\")\n",
    "print(f\"Late Season: {stab['late_season_accuracy']:.1%}\")\n",
    "print(f\"Stability Score: {stab['stability_score']:.3f}\")\n",
    "print(f\"Is Stable: {'‚úÖ YES' if stab['is_stable'] else '‚ùå NO'}\")\n",
    "print()\n",
    "\n",
    "if executive_summary['market_competitiveness'].get('has_data'):\n",
    "    print(\"üí∞ MARKET COMPETITIVENESS\")\n",
    "    print(\"-\" * 30)\n",
    "    market = executive_summary['market_competitiveness']\n",
    "    print(f\"Model vs Market: {market['model_vs_market']:+.1%}\")\n",
    "    print(f\"Has Edge: {'‚úÖ YES' if market['has_edge'] else '‚ùå NO'}\")\n",
    "    if 'brier_improvement' in market:\n",
    "        print(f\"Brier Improvement: {market['brier_improvement']:+.4f}\")\n",
    "    print()\n",
    "\n",
    "print(\"üöÄ FINAL RECOMMENDATION\")\n",
    "print(\"=\" * 30)\n",
    "rec = executive_summary['final_recommendation']\n",
    "print(f\"Decision: {rec['recommendation']}\")\n",
    "print(f\"Score: {rec['score']:.1%}\")\n",
    "print(f\"Criteria Met: {rec['criteria_met']}\")\n",
    "print()\n",
    "\n",
    "print(\"‚úÖ Analysis Complete!\")\n",
    "print(f\"üìÅ Charts exported to: {CHARTS_PATH}\")\n",
    "print(f\"üìã Summary saved to: {summary_filename}\")\n",
    "\n",
    "logger.info(\"üéØ Model validation analysis completed successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìã Analysis Complete!\n",
    "\n",
    "This comprehensive model validation analysis provides:\n",
    "\n",
    "‚úÖ **Prediction Accuracy Assessment** - Overall performance vs benchmarks  \n",
    "‚úÖ **Calibration Quality Analysis** - Probability reliability and Brier score  \n",
    "‚úÖ **Temporal Stability Evaluation** - Performance consistency over time  \n",
    "‚úÖ **Market Competitiveness Analysis** - Model vs betting market comparison  \n",
    "‚úÖ **Executive Summary** - Go/no-go deployment recommendations  \n",
    "\n",
    "**Charts and summary exported to:** `models/experiments/charts/`\n",
    "\n",
    "---\n",
    "\n",
    "*Model Validation Analysis - Part 4 of Specialized Notebooks Pipeline*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hockey-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}